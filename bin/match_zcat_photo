#!/usr/bin/env python

"""Match data release redshift catalogs against the original photometric
(target) catalogs.

time /global/u2/i/ioannis/code/desihub/desispec/bin/match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji --mp 1 -o /global/cscratch1/sd/ioannis/photocatalog --targetphot --validate-files

"""
import os, sys, argparse, pdb
from glob import glob
import numpy as np
import fitsio
import multiprocessing
import astropy
from astropy.io import fits
from astropy.table import Table, vstack
from desispec.io import specprod_root
from desimodel.footprint import radec2pix

from desiutil.log import get_logger, DEBUG
log = get_logger(DEBUG)

desi_root = os.environ.get('DESI_ROOT')
fiberassign_dir = os.path.join(desi_root, 'target', 'fiberassign', 'tiles', 'trunk')
dr9dir = '/global/cfs/cdirs/cosmo/data/legacysurvey/dr9'

TARGETINGBITCOLS = [
    'CMX_TARGET',
    'DESI_TARGET', 'BGS_TARGET', 'MWS_TARGET',
    'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
    'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
    'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
    'SCND_TARGET',
    'SV1_SCND_TARGET', 'SV2_SCND_TARGET', 'SV3_SCND_TARGET',
    ]
    
def _cache_one_catalog(args):
    return cache_one_catalog(*args) 

def cache_one_catalog(cachefile):
    log.info('Reading and caching {}'.format(cachefile))
    if '.ecsv' in cachefile:
        cat = Table.read(cachefile)
        key = 'TOO'
        #if 'sv3' in cachefile:
        #    key = 'TOO_sv3'
        #else:
        #    key = 'TOO_main'
    else:
        cat = fitsio.read(cachefile, columns='TARGETID') # just targetid
        #cat = Table(fitsio.read(cachefile))
        key = cachefile
    return {key: cat}

def _targets_one_healpix(args):
    return targets_one_healpix(*args)

def _targets_one_tile(args):
    return targets_one_tile(*args)

def _tractor_one_brick(args):
    return tractor_one_brick(*args)

def tractor_one_brick(cat):
    """Retrieve the Tractor catalog for all the objects in this catalog (one brick)."""

    assert(np.all(cat['PHOTSYS'] == cat['PHOTSYS'][0]))
    assert(np.all(cat['BRICKNAME'] == cat['BRICKNAME'][0]))

    # find the catalog
    photsys = cat['PHOTSYS'][0]
    brick = cat['BRICKNAME'][0]

    if photsys == 'S':
        region = 'south'
    elif photsys == 'N':
        region = 'north'

    #raslice = np.array(['{:06d}'.format(int(ra*1000))[:3] for ra in cat['RA']])
    tractorfile = os.path.join(dr9dir, region, 'tractor', brick[:3], 'tractor-{}.fits'.format(brick))

    if not os.path.isfile(tractorfile):
        log.warning('Unable to find Tractor catalog {}'.format(tractorfile))
        raise IOError

    objid = fitsio.read(tractorfile, columns='OBJID')
    I = np.where(np.isin(objid, cat['BRICK_OBJID']))[0]

    tractor = Table(fitsio.read(tractorfile, rows=I))

    # sort explicitly in order to ensure order
    srt = np.hstack([np.where(objid == tractor['objid'])[0] for objid in cat['BRICK_OBJID']])
    tractor = tractor[srt]
    assert(np.all(tractor['objid'] == cat['BRICK_OBJID']))

    tractor['targetid'] = cat['TARGETID']

    return tractor

def targets_one_healpix(healpix, zcat, survey, program, tileids, photocache,
                        datamodel, reduxdir, validate_files=False):
    """Build the photometric target catalog for all the objects on a single tile."""

    # check that the spectra exist as a validation step
    if validate_files:
        coaddfile = os.path.join(reduxdir, 'healpix', survey, program, str(healpix//100), str(healpix),
                                 'coadd-{}-{}-{}.fits'.format(survey, program, healpix))
        redrockfile = os.path.join(reduxdir, 'healpix', survey, program, str(healpix//100), str(healpix),
                                   'redrock-{}-{}-{}.fits'.format(survey, program, healpix))
        if not os.path.isfile(coaddfile):
            log.warning('Missing {}'.format(coaddfile))
        if not os.path.isfile(redrockfile):
            log.warning('Missing {}'.format(redrockfile))

    # build the full photometric catalog
    targetdirs, TOO = [], None
    for tileid in tileids:
        _targetdirs, _TOO = get_targetdirs(tileid, photocache)
        if _TOO is not None:
            TOO = _TOO
        targetdirs.append(_targetdirs)
    targetdirs = np.hstack(targetdirs)
    out = read_target_photo(zcat, photocache, datamodel, targetdirs, TOO=TOO)

    srt = np.hstack([np.where(tid == out['TARGETID'])[0] for tid in zcat['TARGETID']])
    out = out[srt]    
    assert(np.all(out['TARGETID'] == zcat['TARGETID']))

    return out

def targets_one_tile(tileid, zcat, photocache, datamodel, reduxdir, coadd_type, validate_files=False):
    """Build the photometric target catalog for all the objects on a single tile."""

    # check that the spectra exist as a validation step
    if validate_files:
        assert(len(np.unique(zcat['LASTNIGHT'])) == 1) # unique?
        lastnight = str(zcat['LASTNIGHT'][0])
        for petal in set(zcat['PETAL_LOC']):
            coaddfile = os.path.join(reduxdir, 'tiles', coadd_type, str(tileid), lastnight,
                                     'coadd-{}-{}-thru{}.fits'.format(str(petal), tileid, lastnight))
            redrockfile = os.path.join(reduxdir, 'tiles', coadd_type, str(tileid), lastnight,
                                       'redrock-{}-{}-thru{}.fits'.format(str(petal), tileid, lastnight))
            if not os.path.isfile(coaddfile):
                log.warning('Missing {}'.format(coaddfile))
            if not os.path.isfile(redrockfile):
                log.warning('Missing {}'.format(redrockfile))
    
    # build the full photometric catalog
    targetdirs, TOO = get_targetdirs(tileid, photocache)
    out = read_target_photo(zcat, photocache, datamodel, targetdirs, TOO=TOO)

    return out

def get_targetdirs(tileid, photocache):
    """Get the targets catalog used to build a given fiberassign catalog."""
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    if not os.path.isfile(fiberfile):
        fiberfile = fiberfile.replace('.gz', '')
        if not os.path.isfile(fiberfile):
            log.warning('Fiber assignment file {} not found!'.format(fiberfile))
    log.debug('Reading {} header.'.format(fiberfile))
    # old versions of fitsio can't handle CONTINUE header cards!
    #fahdr = fitsio.read_header(fiberfile, ext=0)
    fahdr = fits.getheader(fiberfile, ext=0)
    targetdirs = [fahdr['TARG']]
    for moretarg in ['TARG2', 'TARG3', 'TARG4']:
        if moretarg in fahdr:
            if 'gaia' not in fahdr[moretarg]: # skip
                targetdirs += [fahdr[moretarg]]
    if 'SCND' in fahdr:
        if fahdr['SCND'].strip() != '-':
            targetdirs += [fahdr['SCND']]

    for ii, targetdir in enumerate(targetdirs):
        # for secondary targets, targetdir can be a filename
        if targetdir[-4:] == 'fits': # fragile...
            targetdir = os.path.dirname(targetdir)
        if not os.path.isdir(targetdir):
            # can be a KPNO directory!
            if 'DESIROOT' in targetdir:
                targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
            if targetdir[:6] == '/data/':
                targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
            
        if os.path.isdir(targetdir):
            log.debug('Found targets directory {}'.format(targetdir))
            targetdirs[ii] = targetdir
        else:
            log.warning('Targets directory {} not found.'.format(targetdir))
            continue

    # any ToOs?
    if 'TOO' in fahdr:
        TOO = photocache['TOO']
        ## special case, fragile!
        #if 'sv3' in fahdr['TOO']:
        #    TOO = photocache['TOO_sv3']
        #else:
        #    TOO = photocache['TOO_main']
    else:
        TOO = None

    targetdirs = np.unique(np.hstack(targetdirs))
        
    return targetdirs, TOO

def read_target_photo(zcat, photocache, datamodel, targetdirs, TOO=None):
    """For a given tile, given a set of target directories and (optionally) a
    TOOfile used to build the set of targets, read and stack all the photometric
    targeting information for all objects.

    """
    # initialize the output catalog
    out = Table(np.hstack(np.repeat(datamodel, len(zcat))))
    out['TARGETID'] = zcat['TARGETID']

    targetdirs = np.unique(targetdirs)
    
    photo, photofiles = [], []
    for targetdir in targetdirs:
        # Handle secondary targets, which have a different data model;
        # update on 2021 July 31: these catalogs are missing DR9
        # photometry, so we have to skip them for now.
        if 'secondary' in targetdir:
            #continue                    
            if 'sv1' in targetdir: # special case
                if 'dedicated' in targetdir:
                    targetfiles = glob(os.path.join(targetdir, 'DC3R2_GAMA_priorities.fits'))
                else:
                    targetfiles = glob(os.path.join(targetdir, '*-secondary-dr9photometry.fits'))
            else:
                targetfiles = glob(os.path.join(targetdir, '*-secondary.fits'))
        else:
            alltargetfiles = glob(os.path.join(targetdir, '*-hp-*.fits'))
            filenside = fitsio.read_header(alltargetfiles[0], ext=1)['FILENSID']
            # https://github.com/desihub/desispec/issues/1711
            if np.any(np.isnan(zcat['TARGET_RA'])): # some SV1 targets have nan in RA,DEC
                log.warning('Some RA, DEC are NaN in target directory {}'.format(targetdir))
            notnan = np.isfinite(zcat['TARGET_RA'])
            targetfiles = []
            if np.sum(notnan) > 0:
                pixlist = radec2pix(filenside, zcat['TARGET_RA'][notnan], zcat['TARGET_DEC'][notnan])
                for pix in set(pixlist):
                    # /global/cfs/cdirs/desi/target/catalogs/gaiadr2/0.48.0/targets/sv1/resolve/supp/sv1targets-supp-hp-128.fits doesn't exist...
                    _targetfile = alltargetfiles[0].split('hp-')[0]+'hp-{}.fits'.format(pix)
                    if os.path.isfile(_targetfile):
                        targetfiles.append(_targetfile)

        targetfiles = np.unique(targetfiles)

        if len(targetfiles) == 0:
            continue

        #print(targetfiles)
        for ifile, targetfile in enumerate(targetfiles):
            # If this is a secondary target catalog, use the cache. Also note
            # that secondary target catalogs are missing some or all of the DR9
            # photometry columns we need, so only copy what exists, e.g.,
            #   /global/cfs/cdirs/desi/spectro/redux/everest/healpix/sv3/bright/153/15343/redrock-sv3-bright-15343.fits
            if targetfile in photocache.keys():
                if type(photocache[targetfile]) == astropy.table.Table:
                    I = np.where(np.isin(photocache[targetfile]['TARGETID'], zcat['TARGETID']))[0]
                else:
                    photo_targetid = photocache[targetfile]
                    I = np.where(np.isin(photo_targetid, zcat['TARGETID']))[0]
                    
                log.debug('Matched {} targets in {}'.format(len(I), targetfile))
                if len(I) > 0:
                    if type(photocache[targetfile]) == astropy.table.Table:
                        cachecat = photocache[targetfile][I]
                    else:
                        cachecat = Table(fitsio.read(targetfile, rows=I))
                    
                    _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
                    for col in _photo.colnames: # not all these columns will exist...
                        if col in cachecat.colnames:
                            _photo[col] = cachecat[col]
                    photofiles.append(targetfile)
                    photo.append(_photo)
                continue

            # get the correct extension name or number
            tinfo = fitsio.FITS(targetfile)
            for _tinfo in tinfo:
                extname = _tinfo.get_extname()
                if 'TARGETS' in extname:
                    break
            if extname == '':
                extname = 1
                
            # fitsio does not preserve the order of the rows but we'll sort later.
            photo_targetid = tinfo[extname].read(columns='TARGETID')
            I = np.where(np.isin(photo_targetid, zcat['TARGETID']))[0]
            #if len(I) == 0:
            #    log.warning('No matching targets!')
            #    raise ValueError
            
            log.debug('Matched {} targets in {}'.format(len(I), targetfile))
            if len(I) > 0:
                photo1 = tinfo[extname].read(rows=I)
                # Columns can be out of order, so sort them here based on the
                # data model so we can stack below.
                _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
                for col in _photo.colnames: # all these columns should exist...
                    if col in photo1.dtype.names:
                        _photo[col] = photo1[col]
                    else:
                        log.debug('Skipping missing column {} from {}'.format(col, targetfile))
                del photo1
                photofiles.append(targetfile)
                photo.append(_photo)

    # handle ToO targets
    if TOO is not None:
        I = np.where(np.isin(TOO['TARGETID'], zcat['TARGETID']))[0]
        log.debug('Matched {} TOO targets'.format(len(I)))
        if len(I) > 0:
            cachecat = TOO[I]
            _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
            for col in _photo.colnames: # not all these columns will exist...
                if col in cachecat.colnames:
                    _photo[col] = cachecat[col]
            photofiles.append('TOO')
            photo.append(_photo)

    # backup programs have no target catalog photometry at all
    if len(photo) == 0:
        log.warning('No photometry found at all!')
        photo = [out] # empty set

    # np.hstack will sometimes complain even if the tables are identical...
    #photo = Table(np.hstack(photo))
    photo = vstack(photo)
    #for col in photo[0].colnames:
    #    if not photo[0][col].dtype == photo[1][col].dtype:
    #        log.warning('Mismatching dtype in column {}'.format(col))

    # make sure there are no duplicates...?
    _, uindx = np.unique(photo['TARGETID'], return_index=True) 
    photo = photo[uindx]
    assert(len(np.unique(photo['TARGETID'])) == len(photo))

    # sort explicitly in order to ensure order
    I = np.where(np.isin(out['TARGETID'], photo['TARGETID']))[0]
    srt = np.hstack([np.where(tid == photo['TARGETID'])[0] for tid in out['TARGETID'][I]])
    out[I] = photo[srt]
    
    ## photometric columns can be out of order...
    #for col in photo.colnames:
    #    #print(col)
    #    out[col][I] = photo[srt][col]

    return out

def main():

    p = argparse.ArgumentParser()
    p.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    p.add_argument('-o', '--outdir', type=str, required=True, help='output directory file')
    p.add_argument('--mp', type=int, default=1, help='number of multiprocessing cores')
    p.add_argument('--validate-files', action='store_true', help='Validate individual files.')
    p.add_argument('--get-secondary-targetdirs', action='store_true', help='Figure out which secondary target catalogs actually were used to design tiles.')
    p.add_argument('--targetphot', action='store_true', help='Build the photo-targets catalogs.')
    p.add_argument('--tractorphot', action='store_true', help='Build the photo-tractor catalogs.')
    p.add_argument('--overwrite-targetphot', action='store_true', help='Overwrite existing photo-targets files.')
    p.add_argument('--overwrite-tractorphot', action='store_true', help='Overwrite existing photo-tractor files.')
    
    args = p.parse_args()
    log = get_logger()

    if args.reduxdir is None:
        args.reduxdir = specprod_root()

    if not os.path.isdir(args.outdir):
        os.makedirs(args.outdir, exist_ok=True)

    # for testing
    rand = np.random.RandomState(seed=1)
    rand = None

    if args.targetphot:

        # Fragile! Read a single row of a single target catalog in order to get the
        # correct photometric data model. Add all the _TARGET columns at the end
        datamodel = Table(fitsio.read('/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/resolve/dark/targets-dark-hp-0.fits', rows=0))
        for col in datamodel.colnames:
            if '_TARGET' in col:
                datamodel.remove_column(col)
            else:
                datamodel[col] = np.zeros(datamodel[col].shape, dtype=datamodel[col].dtype)
        for col in TARGETINGBITCOLS:
            datamodel[col] = np.zeros(1, dtype=np.int64)
        
        if args.get_secondary_targetdirs:
            fiberfiles = np.hstack(glob(os.path.join(fiberassign_dir, '???', 'fiberassign-*.fits*')))
            #I = rand.choice(len(fiberfiles), size=50, replace=False) # for testing
            #fiberfiles = fiberfiles[I]
            targetdirs = []
            for ii, fiberfile in enumerate(fiberfiles):
                if ii % 500 == 0:
                    log.info('Working on fiberfile {}/{}'.format(ii, len(fiberfiles)))
                fahdr = fits.getheader(fiberfile, ext=0)        
                if 'SCND' in fahdr:
                    if fahdr['SCND'].strip() != '-':
                        targetdirs += [fahdr['SCND']]
            targetdirs = np.unique(np.hstack(targetdirs))
            final_targetdirs = []
            for ii, targetdir in enumerate(targetdirs):
                # can be a KPNO directory!
                if 'DESIROOT' in targetdir:
                    targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
                if targetdir[:6] == '/data/':
                    targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
                if os.path.isdir(targetdir):
                    targetdir = glob(os.path.join(targetdir, '*.fits'))
                for targetdir1 in np.atleast_1d(targetdir):                
                    if os.path.isfile(targetdir1):
                        log.info('Found secondary targets catalog {}'.format(targetdir1))
                        final_targetdirs.append(targetdir1)
                    else:
                        log.warning('Targets directory {} not found.'.format(targetdir))
            log.info(np.unique(final_targetdirs))
        
        # To speed things up, read and cache all the large secondary target catalogs and
        # the TOO files *once*. Now, not all these catalogs are actually used but we
        # don't know which ones yet until we read all the fiberassign headers.
        ##print('Hack! Only read one secondary catalog!')
        #cachefiles = np.hstack([
        #    glob('/global/cfs/cdirs/desi/target/catalogs/dr9/*/targets/*/secondary/*/*targets-*-secondary*.fits'),
        #    #glob('/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/*targets-*-secondary*.fits'),
        #    glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
        #    ])
    
        #cachefiles = np.hstack([
        #    ['/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        #     #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary-dr9photometry.fits',
        #     #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        #     #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        #     #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        #     #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/bright/targets-bright-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/bright/targets-bright-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/dark/targets-dark-secondary.fits',
        #     '/global/cfs/cdirs/desi/target/secondary/sv1/dedicated/0.49.0/DC3R2_GAMA_priorities.fits'],
        #    glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
        #    ])
        
        cachefiles = np.hstack([
                ['/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits'],
                glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')])
        
        mpargs = []
        for cachefile in cachefiles:
            if cachefile.replace('.fits', '-dr9photometry.fits') in cachefiles and '.ecsv' not in cachefile: # just read the photometry
                continue
            mpargs.append([cachefile])
            
        if args.mp > 1:
            with multiprocessing.Pool(args.mp) as P:
                photocache1 = P.map(_cache_one_catalog, mpargs)
        else:
            photocache1 = [cache_one_catalog(mparg[0]) for mparg in mpargs]
        
        photocache = {}
        for _photocache1 in photocache1:
            for key in _photocache1.keys():
                if key in photocache.keys():
                    photocache[key] = vstack((_photocache1[key], photocache[key]), metadata_conflicts='silent') # stack the TOO catalogs
            photocache.update(_photocache1)

        # handle the zpix catalogs
        zprefix = 'zpix'
        columns = ['TARGETID', 'HEALPIX', 'TARGET_RA', 'TARGET_DEC']
        
        zcatfiles = sorted(set(glob(os.path.join(args.reduxdir, 'zcatalog', '{}-*.fits'.format(zprefix)))))
        zcatfiles = ['/global/cfs/cdirs/desi/spectro/redux/fuji/zcatalog/zpix-sv1-dark.fits']
        for zcatfile in zcatfiles:
            outfile = os.path.join(args.outdir, os.path.basename(zcatfile).replace('{}-'.format(zprefix), '{}-targetphot-'.format(zprefix)))        
            if os.path.isfile(outfile) and not args.overwrite_targetphot:
                log.info('Output file {} exists; use --overwrite-targetphot'.format(outfile))
                continue

            log.info('Working on redshift catalog {}'.format(zcatfile))            
            zcat = Table(fitsio.read(zcatfile, 'ZCATALOG', columns=columns))
            tileids = np.unique(fitsio.read(zcatfile, 'EXP_FIBERMAP', columns='TILEID'))

            hdr = fitsio.read_header(zcatfile, ext='ZCATALOG')
            survey, program = hdr['SURVEY'], hdr['PROGRAM']
            
            # multiprocess over healpixels
            mpargs = [[healpix, zcat[healpix == zcat['HEALPIX']], survey, program, tileids, photocache, datamodel,
                       args.reduxdir, args.validate_files] for healpix in set(zcat['HEALPIX'])]
                    
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    out = P.map(_targets_one_healpix, mpargs)
            else:
                out = [targets_one_healpix(*mparg) for mparg in mpargs]
    
            # stack and write out
            try:
                out = Table(np.hstack(out))

                # skies and stuck positioners can appear multiple times
                #aa = out['TARGETID']
                #aa[~np.in1d(np.arange(len(aa)), np.unique(aa, return_index=True)[1], assume_unique=True)]
                _, uindx = np.unique(out['TARGETID'], return_index=True)
                out = out[uindx]
                
                srt = np.hstack([np.where(tid == out['TARGETID'])[0] for tid in zcat['TARGETID']])
                out = out[srt]    
                assert(np.all(out['TARGETID'] == zcat['TARGETID']))
            except:
                pdb.set_trace()
            out.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {} objects to {}'.format(len(out), outfile))
            out.write(outfile, overwrite=True)

            del out
    
        # handle the ztile catalogs
        zprefix = 'ztile'
        basecolumns = ['TARGETID', 'TILEID', 'PETAL_LOC', 'TARGET_RA', 'TARGET_DEC']
        
        #for coadd_type in ['pernight']:
        for coadd_type in ['cumulative', 'perexp', 'pernight']:
            log.info('Working on coadd type {}'.format(coadd_type))
            zcatfiles = sorted(set(glob(os.path.join(args.reduxdir, 'zcatalog', '{}-*-{}.fits'.format(zprefix, coadd_type)))))
            #print('Hack!')
            #pdb.set_trace()
            #zcatfiles = [zcatfiles[3]]
            #zcatfiles = ['/global/cfs/cdirs/desi/spectro/redux/fuji/zcatalog/ztile-special-dark-{}.fits'.format(coadd_type)]
            zcatfiles = ['/global/cfs/cdirs/desi/spectro/redux/fuji/zcatalog/ztile-sv2-bright-cumulative.fits']
            for zcatfile in zcatfiles:
                outfile = os.path.join(args.outdir, os.path.basename(zcatfile).replace('{}-'.format(zprefix), '{}-targetphot-'.format(zprefix)))
                if os.path.isfile(outfile) and not args.overwrite_targetphot:
                    log.info('Output file {} exists; use --overwrite-targetphot'.format(outfile))
                    continue
    
                if coadd_type == 'cumulative':
                    columns = basecolumns + ['LASTNIGHT']
                elif coadd_type == 'perexp':
                    columns = basecolumns + ['NIGHT', 'EXPID']
                elif coadd_type == 'pernight':
                    columns = basecolumns + ['NIGHT']
    
                log.info('Working on redshift catalog {}'.format(zcatfile))
                zcat = Table(fitsio.read(zcatfile, 'ZCATALOG', columns=columns))
                    
                #print('Hack to just one tile!')
                #zcat = zcat[(zcat['TILEID'] > 81101) * (zcat['TILEID'] < 81107)]
                if rand is not None:
                    #zcat = zcat[zcat['TILEID'] == 81108]
                    J = rand.choice(len(zcat), size=100, replace=False)
                    zcat = zcat[J]
                
                # multiprocess over tiles
                mpargs = [[tileid, zcat[tileid == zcat['TILEID']], photocache, datamodel,
                           args.reduxdir, coadd_type, args.validate_files] for tileid in set(zcat['TILEID'])]
                        
                if args.mp > 1:
                    with multiprocessing.Pool(args.mp) as P:
                        out = P.map(_targets_one_tile, mpargs)
                else:
                    out = [targets_one_tile(*mparg) for mparg in mpargs]
    
                # stack and write out
                try:
                    out = Table(np.hstack(out))

                    # Objects *can* be observed multiple times (on different
                    # tiles), especially skies and stuck positioners, so handle
                    # that here.

                    _, uindx = np.unique(out['TARGETID'], return_index=True)
                    out = out[uindx]
                    
                    srt = np.hstack([np.where(tid == out['TARGETID'])[0] for tid in zcat['TARGETID']])
                    #srt[~np.in1d(np.arange(len(srt)), np.unique(srt, return_index=True)[1], assume_unique=True)]
                    
                    out = out[srt]    
                    assert(np.all(out['TARGETID'] == zcat['TARGETID']))
                except:
                    pdb.set_trace()                    
                out.meta['EXTNAME'] = 'TARGETPHOT'
                log.info('Writing {} objects to {}'.format(len(out), outfile))
                out.write(outfile, overwrite=True)

                del out

                pdb.set_trace()
        
    if args.tractorphot:

        targetfiles = sorted(set(glob(os.path.join(args.outdir, '*-targetphot-*.fits'))))
        for targetfile in targetfiles:

            log.info('Working on {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile))

            # need to instantiate the data model here
            cat = cat[np.where((cat['PHOTSYS'] != '')*(cat['MORPHTYPE'] != ''))[0]] # need to identify skies!
            cat = cat[:5000]

            # multiprocess over bricks
            mpargs = [[cat[brick == cat['BRICKNAME']]] for brick in set(cat['BRICKNAME'])]
                        
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    out = P.map(_tractor_one_brick, mpargs)
            else:
                out = [tractor_one_brick(mparg[0]) for mparg in mpargs]

            out = Table(np.hstack(out))
            pdb.set_trace()
    
        ## Fragile! Read a single row of a single target catalog in order to get the
        ## correct photometric data model. Add all the _TARGET columns at the end
        #datamodel = Table(fitsio.read('/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/resolve/dark/targets-dark-hp-0.fits', rows=0))
        #for col in datamodel.colnames:
        #    if '_TARGET' in col:
        #        datamodel.remove_column(col)
        #    else:
        #        datamodel[col] = np.zeros(datamodel[col].shape, dtype=datamodel[col].dtype)
        #for col in TARGETINGBITCOLS:
        #    datamodel[col] = np.zeros(1, dtype=np.int64)

        #out.meta['EXTNAME'] = 'TRACTORPHOT'
        
if __name__ == '__main__':
    main()
