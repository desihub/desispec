#!/usr/bin/env python

"""Match data release redshift catalogs against the original photometric
(target) catalogs.

Sequence of commands for a given release (here, fuji)
1. Gather targeting photometry from the individual redshift catalogs.

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --targetphot

   Output files:
    targetphot-fuji.fits
    targetphot-zcat-fuji.fits
    targetphot-missing-fuji.fits

2. Gather Tractor photometry.

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --tractorphot

   Output files:
     tractorphot-nside4-hp???-fuji.fits

3. Validate Tractor photometry

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --validate-tractorphot

   Output files:
     tractorphot-validate-fuji.fits

4. Gather targeting and Tractor photometry for *potential* targets (going back to the original fiberassign files).

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --targetphot --potential-targets
   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --tractorphot --potential-targets

   Output files:
     targetphot-potential-targets-fuji.fits
     targetphot-potential-targets-zcat-fuji.fits
     targetphot-potential-targets-missing-fuji.fits
     tractorphot-potential-targets-nside4-hp???-fuji.fits

"""
import os, sys, argparse, pdb
from glob import glob
import numpy as np
import fitsio
import multiprocessing
import healpy as hp

import astropy
from astropy.io import fits
from astropy.table import Table, vstack, join

from desispec.io import specprod_root
from desimodel.footprint import radec2pix
from desitarget.targets import decode_targetid
from desiutil.brick import brickname

from desiutil.log import get_logger, DEBUG
log = get_logger()#DEBUG)

desi_root = os.environ.get('DESI_ROOT')
fiberassign_dir = os.path.join(desi_root, 'target', 'fiberassign', 'tiles', 'trunk')

def _resolve(cat):
    # "Resolve" whether the target is in the north or south. The
    # algorithm here is taken from desitarget.targets.resolve, which was
    # written by ADM. (Note we can't use that method directly because we
    # don't necessarily have photsys or release).
    from desitarget.geomask import is_in_gal_box, pixarea2nside
    from desitarget.io import desitarget_resolve_dec
    split = desitarget_resolve_dec()

    nside = pixarea2nside(1)
    theta, phi = np.radians(90-cat['TARGET_DEC']), np.radians(cat['TARGET_RA'])
    pixnum = hp.ang2pix(nside, theta, phi, nest=True)
    # ADM find the pixels north of the Galactic plane...
    allpix = np.arange(hp.nside2npix(nside))
    theta, phi = hp.pix2ang(nside, allpix, nest=True)
    ra, dec = np.degrees(phi), 90-np.degrees(theta)
    pixn = is_in_gal_box([ra, dec], [0., 360., 0., 90.], radec=True)
    # ADM which objects are in pixels north of the Galactic plane.
    galn = pixn[pixnum]
    return (cat['TARGET_DEC'] >= split) & galn # True=North

def _get_brickname(args):
    return get_brickname(*args)

def get_brickname(bricks, ra, dec):
    I = (ra >= bricks['RA1']) * (ra <= bricks['RA2']) * (dec >= bricks['DEC1']) * (dec <= bricks['DEC2'])
    return bricks['BRICKNAME'][I][0]
    
def _read_one_zcat(args):
    return read_one_zcat(*args) 

def read_one_zcat(catfile):
    log.info('Reading {}'.format(catfile))
    hdr = fitsio.read_header(catfile, ext='ZCATALOG')
    survey, program = hdr['SURVEY'], hdr['PROGRAM']
    cat = Table(fitsio.read(catfile, ext='ZCATALOG', columns=['TARGETID', 'TILEID', 'TARGET_RA', 'TARGET_DEC', 'PETAL_LOC']))
    cat['SURVEY'] = survey
    cat['PROGRAM'] = program
    return cat

def _read_one_potential_targets(args):
    return read_one_potential_targets(*args) 

def read_one_potential_targets(tileid):
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    #log.info('Reading {}'.format(fiberfile))
    targetid = fitsio.read(fiberfile, ext='POTENTIAL_ASSIGNMENTS', columns='TARGETID')
    targetid = np.unique(targetid)
    # remove skies
    #_, _, _, _, sky, _ = decode_targetid(targetid)
    objid, brickid, release, mock, sky, gaia = decode_targetid(targetid)
    keep = (sky == 0) * (targetid > 0)
    out = Table()
    if np.sum(keep) == 0:
        return out
    out['TARGETID'] = targetid[keep]
    out['TILEID'] = tileid

    targets = fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC'])
    out = join(out, targets, keys='TARGETID', join_type='left')
    
    # Old code to populate the brick columns. However, this procedure is not
    # correct for secondary targets, so don't do it.
    #from desitarget.io import release_to_photsys
    #bricks = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'))
    #out['BRICKID'] = brickid[keep]
    #out['OBJID'] = objid[keep]
    #out['RELEASE'] = release[keep]
    #out['BRICKNAME'] = np.zeros(len(out), dtype=bricks['BRICKNAME'].dtype)
    #out['PHOTSYS'] = np.zeros(len(out), dtype='U1')
    #idr9 = np.where((out['RELEASE'] > 9000) * (out['RELEASE'] < 9050))[0]
    #if len(idr9) > 0:
    #    photsys = release_to_photsys(out['RELEASE'][idr9])
    #    out['PHOTSYS'][idr9] = photsys
    #    for bid in set(out['BRICKID'][idr9]):
    #        J = bid == bricks['BRICKID']
    #        I = np.where(bid == out['BRICKID'][idr9])[0]
    #        out['BRICKNAME'][idr9[I]] = bricks['BRICKNAME'][J]
    return out

def _cache_one_catalog(args):
    return cache_one_catalog(*args) 

def cache_one_catalog(cachefile):
    log.info('Caching {}'.format(cachefile))
    if '.ecsv' in cachefile:
        cat = Table.read(cachefile, guess=False, format='ascii.ecsv')
        key = 'TOO'
    else:
        cat = fitsio.read(cachefile, columns='TARGETID') # just targetid
        key = cachefile
    return {key: cat}

def _tractorphot_one(args):
    return tractorphot_one(*args)

def tractorphot_one(cat, racolumn='TARGET_RA', deccolumn='TARGET_DEC'):
    """Retrieve the Tractor catalog for all the objects in one brick."""
    from desispec.io.photo import gather_tractorphot
    tractorphot = gather_tractorphot(cat, racolumn=racolumn, deccolumn=deccolumn)
    return tractorphot

def _targetphot_one(args):
    return targetphot_one(*args)

def targetphot_one(input_cat, tileids, photocache=None, racolumn='TARGET_RA',
                   deccolumn='TARGET_DEC'):
    """Build the targetphot catalog for all the objects on a single tile."""
    from desispec.io.photo import gather_targetphot, gather_targetdirs

    # Get the unique list of targetdirs
    targetdirs = np.unique(np.hstack([gather_targetdirs(tileid) for tileid in set(input_cat['TILEID'])]))
    
    targetphot = gather_targetphot(input_cat, targetdirs=targetdirs, photocache=photocache,
                                   racolumn=racolumn, deccolumn=deccolumn)
    return targetphot

def _get_secondary_targetdirs():
    """Scrape all fiberassign headers to find which secondary catalogs are ever used.

    This list of files can then be cached (separately).

    """
    fiberfiles = np.hstack(glob(os.path.join(fiberassign_dir, '???', 'fiberassign-*.fits*')))
    #I = rand.choice(len(fiberfiles), size=50, replace=False) # for testing
    #fiberfiles = fiberfiles[I]
    targetdirs = []
    for ii, fiberfile in enumerate(fiberfiles):
        if ii % 500 == 0:
            log.info('Working on fiberfile {}/{}'.format(ii, len(fiberfiles)))
        fahdr = fits.getheader(fiberfile, ext=0)        
        if 'SCND' in fahdr:
            if fahdr['SCND'].strip() != '-':
                targetdirs += [fahdr['SCND']]
    targetdirs = np.unique(np.hstack(targetdirs))
    final_targetdirs = []
    for ii, targetdir in enumerate(targetdirs):
        # can be a KPNO directory!
        if 'DESIROOT' in targetdir:
            targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
        if targetdir[:6] == '/data/':
            targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
        if os.path.isdir(targetdir):
            targetdir = glob(os.path.join(targetdir, '*.fits'))
        for targetdir1 in np.atleast_1d(targetdir):                
            if os.path.isfile(targetdir1):
                log.info('Found secondary targets catalog {}'.format(targetdir1))
                final_targetdirs.append(targetdir1)
            else:
                log.warning('Targets directory {} not found.'.format(targetdir))
    final_targetdirs = np.unique(final_targetdirs)
    log.debug(final_targetdirs)

    return final_targetdirs

def _build_secondarycache(mp=1):
    """To speed things up, read and cache all the large secondary target catalogs
    and the TOO files *once*. Now, not all these catalogs are actually used but
    we don't know which ones yet until we read all the fiberassign headers.

    """
    # Figure out which secondary target catalogs actually were used to design
    # tiles but only do this one time.
    #cachefiles = _get_secondary_targetdirs()

    #cachefiles = np.hstack([
    #    glob('/global/cfs/cdirs/desi/target/catalogs/dr9/*/targets/*/secondary/*/*targets-*-secondary*.fits'),
    #    #glob('/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/*targets-*-secondary*.fits'),
    #    glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
    #    ])
    
    cachefiles = np.hstack([
        [#'/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
         #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary-dr9photometry.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary.fits',
         #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
         #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
         #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/bright/targets-bright-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/targets-dark-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/bright/targets-bright-secondary.fits',
         '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/dark/targets-dark-secondary.fits',
         '/global/cfs/cdirs/desi/target/secondary/sv1/dedicated/0.49.0/DC3R2_GAMA_priorities.fits'],
        glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
        ])
    
    #cachefiles = np.hstack([
    #        ['/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
    #         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
    #         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
    #         ],
    #        glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')])
    
    mpargs = []
    for cachefile in np.unique(cachefiles):
        #if cachefile.replace('.fits', '-dr9photometry.fits') in cachefiles and '.ecsv' not in cachefile: # just read the photometry
        #    continue
        mpargs.append([cachefile])
    if mp > 1:
        with multiprocessing.Pool(mp) as P:
            photocache1 = P.map(_cache_one_catalog, mpargs)
    else:
        photocache1 = [cache_one_catalog(mparg[0]) for mparg in mpargs]
    
    photocache = {}
    for _photocache1 in photocache1:
        for key in _photocache1.keys():
            if key in photocache.keys():
                photocache[key] = vstack((_photocache1[key], photocache[key]), metadata_conflicts='silent') # stack the TOO catalogs
        photocache.update(_photocache1)

    return photocache

def main():

    p = argparse.ArgumentParser()
    p.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    p.add_argument('-o', '--outdir', type=str, required=True, help='output directory file')
    p.add_argument('--outprefix', type=str, required=True, help='output file prefix')
    p.add_argument('--mp', type=int, default=1, help='number of multiprocessing cores')
    p.add_argument('--filenside', type=int, default=4, help='healpix nside for tractorphot catalogs.')
    p.add_argument('--no-secondarycache', action='store_true', help='Do not cache the secondary target catalogs.')
    p.add_argument('--ntest', type=int, default=None, help='Test on a randomly selected sample of NTEST objects.')
    p.add_argument('--potential-targets', action='store_true', help='Gather photometry for the potential targets.')
    p.add_argument('--targetphot', action='store_true', help='Build the photo-targets catalogs.')
    p.add_argument('--tractorphot', action='store_true', help='Build the photo-tractor catalogs.')
    p.add_argument('--validate-tractorphot', action='store_true', help='Validate the photo-tractor catalogs.')
    p.add_argument('--overwrite', action='store_true', help='Overwrite existing photo-targets files.')
    
    args = p.parse_args()
    log = get_logger()

    if args.reduxdir is None:
        args.reduxdir = specprod_root()

    if not os.path.isdir(args.outdir):
        os.makedirs(args.outdir, exist_ok=True)
    if not os.path.isdir(os.path.join(args.outdir, 'tractorphot')):
        os.makedirs(os.path.join(args.outdir, 'tractorphot'), exist_ok=True)

    if args.ntest is not None:
        rand = np.random.RandomState(seed=1)
    else:
        rand = None

    # Build the targetphot catalog.
    if args.targetphot:
        if args.potential_targets:        
            outfile = os.path.join(args.outdir, 'targetphot-potential-targets-{}.fits'.format(args.outprefix))
        else:
            outfile = os.path.join(args.outdir, 'targetphot-{}.fits'.format(args.outprefix))
            
        if os.path.isfile(outfile) and not args.overwrite:
            log.info('Output file {} exists; use --overwrite'.format(outfile))
            return

        # Optionally cache the targetids of all the secondary target
        # catalogs. This cache is used in desispec.io.photo.gather_targetphot.
        if args.no_secondarycache:
            photocache = {}
        else:
            photocache = _build_secondarycache(mp=args.mp)
    
        if args.potential_targets:
            # Assume that the nominal/parent zcat catalog has been previously generated.
            parent_zcatfile = os.path.join(args.outdir, 'targetphot-zcat-{}.fits'.format(args.outprefix))
            if not os.path.isfile(parent_zcatfile):
                log.warning('Targeting photometric catalog {} missing'.format(parent_zcatfile))
                raise IOError
            log.info('Reading {}'.format(parent_zcatfile))
            parent_zcat = Table(fitsio.read(parent_zcatfile))

            if rand is not None:
                #parent_zcat = parent_zcat[parent_zcat['TILEID'] == 81108]
                J = rand.choice(len(parent_zcat), size=args.ntest, replace=False)
                parent_zcat = parent_zcat[J]
            tileids = np.unique(parent_zcat['TILEID'])

            # Multiprocess over tiles
            log.info('Dividing the sample into {} unique tiles.'.format(len(tileids)))
            mpargs = [[tileid] for tileid in tileids]
                    
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_potential_targets, mpargs)
            else:
                zcat = [read_one_potential_targets(*mparg) for mparg in mpargs]

            # stack and find unique objects
            zcat = vstack(zcat)
            _, uindx = np.unique(zcat['TARGETID'], return_index=True)
            zcat = zcat[uindx]

            log.info('Found {:,} unique TARGETIDs from {:,} unique tiles.'.format(
                len(zcat), len(set(zcat['TILEID']))))
        else:
            # Read all the zpix catalogs in parallel.
            zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
            #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits'))
            mpargs = []
            for zcatfile in zcatfiles:
                mpargs.append([zcatfile])
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_zcat, mpargs)
            else:
                zcat = [read_one_zcat(mparg[0]) for mparg in mpargs]

            # Stack, remove negative targetids and sky fibers, and then select just
            # the unique TARGETIDs.
            zcat = vstack(zcat)
            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)
            if np.sum(keep) > 0:
                log.info('Keeping {}/{} objects after removing sky targets and stuck positioners)'.format(
                    np.sum(keep), len(zcat)))
                zcat = zcat[keep]
            
            _, uindx = np.unique(zcat['TARGETID'], return_index=True)
            zcat = zcat[uindx]
            zcat = zcat[np.argsort(zcat['TARGETID'])]

            if rand is not None:
                J = rand.choice(len(zcat), size=args.ntest, replace=False)
                zcat = zcat[J]
            
            log.info('Found {:,} unique TARGETIDs and {:,} unique tiles from {} redshift catalogs'.format(
                len(zcat), len(set(zcat['TILEID'])), len(zcatfiles)))
    
        if args.potential_targets:
            # Multiprocess over tiles
            log.info('Dividing the sample into {} unique tiles.'.format(len(set(zcat['TILEID']))))
            mpargs = [[zcat[zcat['TILEID'] == tileid], tileid, photocache, 'RA', 'DEC'] for tileid in set(zcat['TILEID'])]
        else:
            # Multiprocess over nside=8 healpixels which is how the
            # non-secondary targeting catalogs are organized on-disk.
            tileids = np.unique(zcat['TILEID'])
            pixels = radec2pix(8, zcat['TARGET_RA'], zcat['TARGET_DEC'])
            log.info('Dividing the sample into {} nside=8 healpixels.'.format(len(set(pixels))))
            mpargs = [[zcat[pixel == pixels], tileids, photocache, 'TARGET_RA', 'TARGET_DEC'] for pixel in set(pixels)]
                    
        if args.mp > 1:
            with multiprocessing.Pool(args.mp) as P:
                out = P.map(_targetphot_one, mpargs)
        else:
            out = [targetphot_one(*mparg) for mparg in mpargs]

        # stack, sort, and write out
        if len(out) > 0:
            out = vstack(out)
            out = out[np.argsort(out['TARGETID'])]

            out.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {:,} objects to {}'.format(len(out), outfile))
            out.write(outfile, overwrite=True)
            del out, photocache
    
            # Also write out the redshift catalog (with a minimal number of columns)
            # used to generate the targetphot file. Note that this intermediate
            # catalog is needed in order to do positional Tractor photometry
            # gathering, below.
            if args.potential_targets:        
                outfile_zcat = os.path.join(args.outdir, 'targetphot-potential-targets-zcat-{}.fits'.format(args.outprefix))
            else:
                outfile_zcat = os.path.join(args.outdir, 'targetphot-zcat-{}.fits'.format(args.outprefix))
            
            zcat.meta['EXTNAME'] = 'ZCATALOG'
            log.info('Writing {:,} objects to {}'.format(len(zcat), outfile_zcat))
            zcat.write(outfile_zcat, overwrite=True)
            del zcat
        
    # Build the tractorphot catalogs.
    if args.tractorphot:
        if args.potential_targets:
            any_outfile = glob(os.path.join(args.outdir, 'tractorphot', 'tractorphot-potential-targets-nside{}-hp*-{}.fits'.format(
                args.filenside, args.outprefix)))
        else:
            any_outfile = glob(os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp*-{}.fits'.format(
                args.filenside, args.outprefix)))
        if len(any_outfile) > 0 and not args.overwrite:
            log.info('One or more tractorphot output files exist; use --overwrite')
            return

        if args.potential_targets:        
            zcatfile = os.path.join(args.outdir, 'targetphot-potential-targets-zcat-{}.fits'.format(args.outprefix))
            targetfile = os.path.join(args.outdir, 'targetphot-potential-targets-{}.fits'.format(args.outprefix))
            racolumn, deccolumn = 'RA', 'DEC'
        else:
            zcatfile = os.path.join(args.outdir, 'targetphot-zcat-{}.fits'.format(args.outprefix))
            targetfile = os.path.join(args.outdir, 'targetphot-{}.fits'.format(args.outprefix))
            racolumn, deccolumn = 'TARGET_RA', 'TARGET_DEC'
            
        if not os.path.isfile(targetfile):
            log.warning('Targeting photometric catalog {} missing'.format(targetfile))
            raise IOError

        if not os.path.isfile(zcatfile):
            log.warning('Targeting redshift catalog {} missing'.format(zcatfile))
            raise IOError

        if rand is not None:
            N = fitsio.FITS(targetfile)[1].get_nrows()
            J = rand.choice(N, size=args.ntest, replace=False)
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile, rows=J))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile, rows=J))
        else:
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
        assert(np.all(cat['TARGETID'] == zcat['TARGETID']))
        
        # Create a mini-cat that we'll use for the searching.
        minicat = cat['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID']#, 'RA', 'DEC']
        minicat[racolumn] = zcat[racolumn]
        minicat[deccolumn] = zcat[deccolumn]

        # Some secondary programs (e.g., 39632961435338613, 39632966921487347)
        # have BRICKNAME!='' & BRICKID!=0, but BRICK_OBJID==0. Unpack those here
        # using decode_targetid.
        fix = np.where((minicat['BRICKNAME'] != '') * (minicat['BRICK_OBJID'] == 0))[0]
        if len(fix) > 0:
            log.info('Inferring BRICK_OBJID for {} objects using decode_targetid'.format(len(fix)))
            fix_objid, fix_brickid, _, _, _, _ = decode_targetid(minicat['TARGETID'][fix])
            assert(np.all(fix_brickid == minicat['BRICKID'][fix]))
            minicat['BRICK_OBJID'][fix] = fix_objid

        inobrickname = np.where(minicat['BRICKNAME'] == '')[0]
        if len(inobrickname) > 0:
            log.info('Inferring brickname for {:,} objects'.format(len(inobrickname)))
            minicat['BRICKNAME'][inobrickname] = brickname(minicat[racolumn][inobrickname],
                                                           minicat[deccolumn][inobrickname])

        assert(np.all(minicat['BRICKNAME'] != ''))

        # Loop over args.filenside healpixels and multiprocess over bricks. We
        # could/should MPI-parallelize over healpixels.
        log.info('Gathering Tractor photometry for {:,} objects'.format(len(minicat)))

        pixels = radec2pix(args.filenside, minicat[racolumn], minicat[deccolumn])
        npix = len(set(pixels))

        missing = []
        for ipix, pixel in enumerate(set(pixels)):
            #if ipix < 3:
            #    continue
            log.info('Working on healpix {}/{} (nside={})'.format(ipix+1, npix, args.filenside))

            I = np.where(pixel == pixels)[0]
            bricknames = minicat['BRICKNAME'][I]

            log.info('Dividing the sample into {} unique bricks.'.format(len(set(bricknames))))
            mpargs = []
            for brick in set(bricknames):
                J = np.where(brick == bricknames)[0]
                mpargs.append([minicat[I[J]], racolumn, deccolumn])
                
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    tractor = P.map(_tractorphot_one, mpargs)
            else:
                tractor = [_tractorphot_one(mparg) for mparg in mpargs]
            tractor = vstack(tractor)

            if len(tractor) > 0:
                # I don't understand why the Tables aren't sorted???
                srt = np.hstack([np.where(tid == tractor['TARGETID'])[0] for tid in minicat['TARGETID'][I]])
                tractor = tractor[srt]
                assert(np.all(cat['TARGETID'][I] == tractor['TARGETID']))
        
                # If there are any objects with missing Tractor photometry, keep track of them here.
                imiss = np.where(tractor['BRICKNAME'] == '')[0]
                if len(imiss) > 0:
                    missing.append(cat[I[imiss]])
                    
                igood = np.where(tractor['BRICKNAME'] != '')[0]
                if len(igood) > 0:
                    if args.potential_targets:
                        outfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-potential-targets-nside{}-hp{:03d}-{}.fits'.format(
                            args.filenside, pixel, args.outprefix))
                    else:
                        outfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                            args.filenside, pixel, args.outprefix))
        
                    tractor.meta['EXTNAME'] = 'TRACTORPHOT'
                    tractor.meta['FILENSID'] = args.filenside
                    log.info('Writing {:,} objects to {}'.format(len(tractor[igood]), outfile))
                    tractor[igood].write(outfile, overwrite=True)
            #del minicat

        if len(missing) > 0:
            missing = vstack(missing)
    
            if args.potential_targets:
                outfile_miss = os.path.join(args.outdir, 'targetphot-potential-targets-missing-{}.fits'.format(args.outprefix))
            else:
                outfile_miss = os.path.join(args.outdir, 'targetphot-missing-{}.fits'.format(args.outprefix))
            missing.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {:,} objects to {}'.format(len(missing), outfile_miss))
            missing.write(outfile_miss, overwrite=True)
    
    # Validate the Tractor photometry against the original redshift catalogs.
    if args.validate_tractorphot:
        outcat = []
        outzcat = []

        missfile = os.path.join(args.outdir, 'targetphot-missing-{}.fits'.format(args.outprefix))
        log.info('Reading {}'.format(missfile))
        miss = Table(fitsio.read(missfile))
        
        zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'zpix-special-*.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits'))        
        for zcatfile in zcatfiles:
            log.info('Validating {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
            hdr = fitsio.read_header(zcatfile, ext=1)
            zcat['SURVEY'] = hdr['SURVEY']
            zcat['PROGRAM'] = hdr['PROGRAM']

            # Toss out skies and stuck positioners.
            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)    
            zcat = zcat[keep]
        
            # read the appropriate Tractor catalog
            pixels = radec2pix(args.filenside, zcat['TARGET_RA'], zcat['TARGET_DEC'])
            log.info('Working on {} pixels'.format(len(set(pixels))))
            for pixel in set(pixels):
                I = np.where(pixel == pixels)[0]
                
                catfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                    args.filenside, pixel, args.outprefix))
                if not os.path.isfile(catfile):
                    log.info('No Tractor photometry found for {} objects in catalog {}'.format(len(I), catfile))
                    # Make sure they're all in the missing catalog.
                    assert(np.sum(np.isin(miss['TARGETID'], zcat['TARGETID'][I])) == len(I))
                    continue
                    
                log.info('Reading {} objects from {}'.format(len(I), catfile))

                _targetids = fitsio.read(catfile, columns='TARGETID')
                targetids = np.intersect1d(_targetids, zcat['TARGETID'][I])

                rows = np.where(np.isin(_targetids, targetids))[0]
                cat = Table(fitsio.read(catfile, rows=rows))
                _zcat = zcat[I][np.isin(zcat['TARGETID'][I], cat['TARGETID'])]
                J = [np.where(tid == cat['TARGETID'])[0] for tid in _zcat['TARGETID']]
                if len(J) == 0: # can happen when testing
                    continue
                J = np.hstack(J)
                cat = cat[J]
                assert(np.all(cat['TARGETID'] == _zcat['TARGETID']))

                diff = (cat['BRICKID'] != _zcat['BRICKID'])
                diff = np.logical_or(diff, cat['OBJID'] != _zcat['BRICK_OBJID'])
                #diff = np.logical_or(diff, cat['flux_g'] != _zcat['FLUX_R'])
                #diff = np.logical_or(diff, cat['flux_r'] != _zcat['FLUX_R'])
                #diff = np.logical_or(diff, cat['flux_z'] != _zcat['FLUX_Z'])
                #diff = np.logical_or(diff, cat['flux_w1'] != _zcat['FLUX_W1'])
                #diff = np.logical_or(diff, cat['flux_w2'] != _zcat['FLUX_W2'])
                diff = np.where(diff)[0]
        
                if np.sum(diff) > 0:
                    outcat.append(cat[diff])
                    outzcat.append(_zcat[diff])
                        
                    #objid, brickid, release, mock, sky, gaia = decode_targetid(_zcat['TARGETID'][diff])
                    #for dd in diff:
                    #    print(_zcat['BRICKID'][dd], cat['brickid'][dd], _zcat['BRICK_OBJID'][dd], cat['objid'][dd])
                    #print(set(release))

        if len(outcat) > 0:
            _outcat = vstack(outcat)
            _outzcat = vstack(outzcat)
            # curse you, astropy!
            import numpy.ma as ma
            for col in _outzcat.colnames:
                if ma.is_masked(_outzcat[col]):
                    _outzcat[col] = ma.getdata(_outzcat[col])

            outcat = _outcat['TARGETID', 'RA', 'DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']
            outzcat = _outzcat['TARGETID', 'TARGET_RA', 'TARGET_DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']

            for col in ['TARGETID', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']:
                if col == 'BRICK_OBJID':
                    outzcat.rename_column(col, 'OBJID_ZCAT')
                else:
                    outzcat.rename_column(col, '{}_ZCAT'.format(col))
            #out = hstack((outcat, outzcat))

            # interlace the columns in the output table
            tractorcols = np.array(outcat.colnames)
            zcols = np.array(outzcat.colnames)
            out = Table()
            for ii in np.arange(len(zcols)):
                out[zcols[ii]] = outzcat[zcols[ii]]
                out[tractorcols[ii]] = outcat[tractorcols[ii]]            
            for col in [
                    'FA_TARGET', 'CMX_TARGET',
                    'DESI_TARGET', 'BGS_TARGET', 'MWS_TARGET',
                    'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
                    'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
                    'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
                    'SCND_TARGET',
                    'SV1_SCND_TARGET', 'SV2_SCND_TARGET', 'SV3_SCND_TARGET',
                    ]:
                out[col] = np.zeros(shape=(1,), dtype=np.int64)

            # add brick_primary, survey, program, and the targeting bits
            out['BRICK_PRIMARY'] = _outcat['BRICK_PRIMARY']
            for col in _outzcat.colnames:
                if '_TARGET' in col and col in _outzcat.colnames:
                    #print(col, _outzcat[col])
                    out[col] = _outzcat[col]

            out['SURVEY'] = _outzcat['SURVEY']
            out['PROGRAM'] = _outzcat['PROGRAM']

            outfile = os.path.join(args.outdir, 'tractorphot-validate-{}.fits'.format(args.outprefix))
            out.write(outfile, overwrite=True)

if __name__ == '__main__':
    main()
