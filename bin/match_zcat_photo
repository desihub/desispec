#!/usr/bin/env python

"""Match data release redshift catalogs against the original photometric
(target) catalogs.

Sequence of commands for a given release (here, fuji)
1. Gather targeting photometry from the individual redshift catalogs.

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --targetphot

   Output files:
    targetphot-fuji.fits
    targetphot-zcat-fuji.fits
    targetphot-missing-fuji.fits

2. Gather Tractor photometry.

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --tractorphot

   Output files:
     tractorphot-nside4-hp???-fuji.fits

3. Validate Tractor photometry

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --validate-tractorphot

   Output files:
     tractorphot-validate-fuji.fits

4. Gather targeting and Tractor photometry for *potential* targets (going back to the original fiberassign files).

   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --targetphot --potential-targets
   time match_zcat_photo --reduxdir /global/cfs/cdirs/desi/spectro/redux/fuji -o /global/cscratch1/sd/ioannis/photocatalog/fuji --outprefix fuji --mp 32 --tractorphot --potential-targets

   Output files:
     targetphot-potential-targets-fuji.fits
     tractorphot-potential-targets-nside4-hp???-fuji.fits

"""
import os, sys, argparse, pdb
from glob import glob
import numpy as np
import fitsio
import multiprocessing
import healpy as hp

import astropy
from astropy.io import fits
from astropy.table import Table, vstack, join
import astropy.units as u
from astropy.coordinates import SkyCoord

from desispec.io import specprod_root
from desimodel.footprint import radec2pix
from desitarget.targets import decode_targetid
from desitarget.io import release_to_photsys, desitarget_resolve_dec
from desitarget.geomask import is_in_gal_box, pixarea2nside

from desiutil.log import get_logger, DEBUG
log = get_logger()#DEBUG)

desi_root = os.environ.get('DESI_ROOT')
fiberassign_dir = os.path.join(desi_root, 'target', 'fiberassign', 'tiles', 'trunk')
dr9dir = '/global/cfs/cdirs/cosmo/data/legacysurvey/dr9'

bricksfile = '/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'

TARGETINGBITCOLS = [
    'CMX_TARGET',
    'DESI_TARGET', 'BGS_TARGET', 'MWS_TARGET',
    'SV1_DESI_TARGET', 'SV1_BGS_TARGET', 'SV1_MWS_TARGET',
    'SV2_DESI_TARGET', 'SV2_BGS_TARGET', 'SV2_MWS_TARGET',
    'SV3_DESI_TARGET', 'SV3_BGS_TARGET', 'SV3_MWS_TARGET',
    'SCND_TARGET',
    'SV1_SCND_TARGET', 'SV2_SCND_TARGET', 'SV3_SCND_TARGET',
    ]

def _resolve(cat, split=desitarget_resolve_dec()):
    # "Resolve" whether the target is in the north or south. The
    # algorithm here is taken from desitarget.targets.resolve, which was
    # written by ADM. (Note we can't use that method directly because we
    # don't necessarily have photsys or release).
    nside = pixarea2nside(1)
    theta, phi = np.radians(90-cat['TARGET_DEC']), np.radians(cat['TARGET_RA'])
    pixnum = hp.ang2pix(nside, theta, phi, nest=True)
    # ADM find the pixels north of the Galactic plane...
    allpix = np.arange(hp.nside2npix(nside))
    theta, phi = hp.pix2ang(nside, allpix, nest=True)
    ra, dec = np.degrees(phi), 90-np.degrees(theta)
    pixn = is_in_gal_box([ra, dec], [0., 360., 0., 90.], radec=True)
    # ADM which objects are in pixels north of the Galactic plane.
    galn = pixn[pixnum]
    return (cat['TARGET_DEC'] >= split) & galn # True=North

def _get_brickname(args):
    return get_brickname(*args)

def get_brickname(bricks, ra, dec):
    I = (ra >= bricks['RA1']) * (ra <= bricks['RA2']) * (dec >= bricks['DEC1']) * (dec <= bricks['DEC2'])
    return bricks['BRICKNAME'][I][0]
    
def _read_one_zcat(args):
    return read_one_zcat(*args) 

def read_one_zcat(catfile):
    log.info('Reading {}'.format(catfile))
    hdr = fitsio.read_header(catfile, ext='ZCATALOG')
    survey, program = hdr['SURVEY'], hdr['PROGRAM']
    cat = Table(fitsio.read(catfile, ext='ZCATALOG', columns=['TARGETID', 'TILEID', 'TARGET_RA', 'TARGET_DEC']))
    cat['SURVEY'] = survey
    cat['PROGRAM'] = program
    return cat

def _read_one_potential_targets(args):
    return read_one_potential_targets(*args) 

def read_one_potential_targets(tileid, bricks):
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    #log.info('Reading {}'.format(fiberfile))
    targetid = fitsio.read(fiberfile, ext='POTENTIAL_ASSIGNMENTS', columns='TARGETID')
    targetid = np.unique(targetid)
    # remove skies
    #_, _, _, _, sky, _ = decode_targetid(targetid)
    objid, brickid, release, mock, sky, gaia = decode_targetid(targetid)
    keep = (sky == 0) * (targetid > 0)
    out = Table()
    if np.sum(keep) == 0:
        return out
    out['TARGETID'] = targetid[keep]
    out['TILEID'] = tileid

    targets = fitsio.read(fiberfile, ext='TARGETS', columns=['TARGETID', 'RA', 'DEC'])
    out = join(out, targets, keys='TARGETID', join_type='left')
    out.rename_column('RA', 'TARGET_RA')
    out.rename_column('DEC', 'TARGET_DEC')
    if False:
        out['BRICKID'] = brickid[keep]
        out['OBJID'] = objid[keep]
        out['RELEASE'] = release[keep]
        out['BRICKNAME'] = np.zeros(len(out), dtype=bricks['BRICKNAME'].dtype)
        out['PHOTSYS'] = np.zeros(len(out), dtype='U1')
        idr9 = np.where((out['RELEASE'] > 9000) * (out['RELEASE'] < 9050))[0]
        if len(idr9) > 0:
            photsys = release_to_photsys(out['RELEASE'][idr9])
            out['PHOTSYS'][idr9] = photsys
            for bid in set(out['BRICKID'][idr9]):
                J = bid == bricks['BRICKID']
                I = np.where(bid == out['BRICKID'][idr9])[0]
                out['BRICKNAME'][idr9[I]] = bricks['BRICKNAME'][J]
    return out

def _cache_one_catalog(args):
    return cache_one_catalog(*args) 

def cache_one_catalog(cachefile):
    log.info('Reading and caching {}'.format(cachefile))
    if '.ecsv' in cachefile:
        cat = Table.read(cachefile)
        key = 'TOO'
    else:
        cat = fitsio.read(cachefile, columns='TARGETID') # just targetid
        key = cachefile
    return {key: cat}

def _tractorphot_one(args):
    return tractorphot_one(*args)

def tractorphot_one(cat, bricks):#, supplement):
    """Retrieve the Tractor catalog for all the objects in this catalog (one brick)."""

    assert(np.all(cat['BRICKNAME'] == cat['BRICKNAME'][0]))
    brick = cat['BRICKNAME'][0]
    
    idr9 = np.where((cat['RELEASE'] > 0) * (cat['BRICKID'] > 0) * (cat['BRICK_OBJID'] > 0))[0]
    ipos = np.delete(np.arange(len(cat)), idr9)
    
    # DR9 targeting photometry exists
    if len(idr9) > 0:
        assert(np.all(cat['PHOTSYS'][idr9] == cat['PHOTSYS'][idr9][0]))
    
        # find the catalog
        photsys = cat['PHOTSYS'][idr9][0]
    
        if photsys == 'S':
            region = 'south'
        elif photsys == 'N':
            region = 'north'
    
        #raslice = np.array(['{:06d}'.format(int(ra*1000))[:3] for ra in cat['RA']])
        tractorfile = os.path.join(dr9dir, region, 'tractor', brick[:3], 'tractor-{}.fits'.format(brick))
    
        if not os.path.isfile(tractorfile):
            log.warning('Unable to find Tractor catalog {}'.format(tractorfile))
            raise IOError

        # Some commissioning and SV targets can have brick_primary==False, so don't require it here.
        #<Table length=1>
        #     TARGETID     BRICKNAME BRICKID BRICK_OBJID RELEASE CMX_TARGET DESI_TARGET   SV1_DESI_TARGET   SV2_DESI_TARGET SV3_DESI_TARGET SCND_TARGET
        #      int64          str8    int32     int32     int16    int64       int64           int64             int64           int64         int64
        #----------------- --------- ------- ----------- ------- ---------- ----------- ------------------- --------------- --------------- -----------
        #39628509856927757  0352p315  503252        4109    9010          0           0 2305843009213693952               0               0           0
        #<Table length=1>
        #     TARGETID         TARGET_RA          TARGET_DEC     TILEID SURVEY PROGRAM
        #      int64            float64            float64       int32   str7    str6
        #----------------- ------------------ ------------------ ------ ------ -------
        #39628509856927757 35.333944142134406 31.496490061792002  80611    sv1  bright

        _tractor = fitsio.read(tractorfile, columns=['OBJID', 'BRICK_PRIMARY'], upper=True)
        #I = np.where(_tractor['BRICK_PRIMARY'] * np.isin(_tractor['OBJID'], cat['BRICK_OBJID']))[0]
        I = np.where(np.isin(_tractor['OBJID'], cat['BRICK_OBJID'][idr9]))[0]

        ## Some secondary programs have BRICKNAME!='' and BRICK_OBJID==0 (i.e.,
        ## not populated). However, there should always be a match here because
        ## we "repair" brick_objid in the main function.
        #if len(I) == 0: 
        #    return Table()

        tractor = Table(fitsio.read(tractorfile, rows=I, upper=True))
    
        # sort explicitly in order to ensure order
        srt = np.hstack([np.where(objid == tractor['OBJID'])[0] for objid in cat['BRICK_OBJID'][idr9]])
        tractor = tractor[srt]
        assert(np.all((tractor['BRICKID'] == cat['BRICKID'][idr9])*(tractor['OBJID'] == cat['BRICK_OBJID'][idr9])))

        tractor['TARGETID'] = cat['TARGETID'][idr9]
        
        return tractor
        
    # use positional matching
    if len(ipos) > 0:
        rad = 1.0 * u.arcsec

        # resolve north/south
        tractorfile_north = os.path.join(dr9dir, 'north', 'tractor', brick[:3], 'tractor-{}.fits'.format(brick))
        tractorfile_south = os.path.join(dr9dir, 'south', 'tractor', brick[:3], 'tractor-{}.fits'.format(brick))
        if os.path.isfile(tractorfile_north) and not os.path.isfile(tractorfile_south):
            tractorfile = tractorfile_north
        elif not os.path.isfile(tractorfile_north) and os.path.isfile(tractorfile_south):
            tractorfile = tractorfile_south
        elif os.path.isfile(tractorfile_north) and os.path.isfile(tractorfile_south):
            if np.median(cat['TARGET_DEC'][ipos]) < desitarget_resolve_dec():
                tractorfile = tractorfile_south
            else:
                tractorfile = tractorfile_north
        elif not os.path.isfile(tractorfile_north) and not os.path.isfile(tractorfile_south):
            return Table()
                
        _tractor = fitsio.read(tractorfile, columns=['RA', 'DEC', 'BRICK_PRIMARY'], upper=True)
        iprimary = np.where(_tractor['BRICK_PRIMARY'])[0] # only primary targets
        _tractor = _tractor[iprimary] 
        coord_tractor = SkyCoord(ra=_tractor['RA']*u.deg, dec=_tractor['DEC']*u.deg)

        # Some targets can appear twice (with different targetids), so
        # to make sure we do it right, we have to loop. Example:
        #
        #     TARGETID    SURVEY PROGRAM     TARGET_RA          TARGET_DEC    OBJID BRICKID RELEASE  SKY  GAIADR    RA     DEC   GROUP BRICKNAME
        #      int64       str7    str6       float64            float64      int64  int64   int64  int64 int64  float64 float64 int64    str8
        # --------------- ------ ------- ------------------ ----------------- ----- ------- ------- ----- ------ ------- ------- ----- ---------
        # 234545047666699    sv1   other 150.31145983340912 2.587887211205909    11  345369      53     0      0     0.0     0.0     0  1503p025
        # 243341140688909    sv1   other 150.31145983340912 2.587887211205909    13  345369      55     0      0     0.0     0.0     0  1503p025

        tractor = []
        for indx_cat, (ra, dec, targetid) in enumerate(zip(cat['TARGET_RA'][ipos], cat['TARGET_DEC'][ipos], cat['TARGETID'][ipos])):
            coord_cat = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
            indx_tractor, d2d, _ = coord_cat.match_to_catalog_sky(coord_tractor)
            if d2d < rad:
                _tractor = Table(fitsio.read(tractorfile, rows=iprimary[indx_tractor], upper=True))
                _tractor['TARGETID'] = targetid
                tractor.append(_tractor)
                #for col in tractor.dtype.names:
                #    supplement[col][indx_cat] = tractor[col][0]
        if len(tractor) > 0:
            return vstack(tractor)
        else:
            return Table()

def _targetphot_one(args):
    return targetphot_one(*args)

def targetphot_one(zcat, photocache, datamodel, reduxdir):
    """Build the photometric target catalog for all the objects on a single tile."""

    # build the full photometric catalog
    targetdirs, TOO = [], None
    for tileid in set(zcat['TILEID']):
        _targetdirs, _TOO = get_targetdirs(tileid, photocache)
        if _TOO is not None:
            TOO = _TOO
        targetdirs.append(_targetdirs)
    targetdirs = np.hstack(targetdirs)
    out = read_target_photo(zcat, photocache, datamodel, targetdirs, TOO=TOO)

    return out

def get_targetdirs(tileid, photocache):
    """Get the targets catalog used to build a given fiberassign catalog."""
    stileid = '{:06d}'.format(tileid)
    fiberfile = os.path.join(fiberassign_dir, stileid[:3], 'fiberassign-{}.fits.gz'.format(stileid))
    if not os.path.isfile(fiberfile):
        fiberfile = fiberfile.replace('.gz', '')
        if not os.path.isfile(fiberfile):
            log.warning('Fiber assignment file {} not found!'.format(fiberfile))
    log.debug('Reading {} header.'.format(fiberfile))
    # old versions of fitsio can't handle CONTINUE header cards!
    #fahdr = fitsio.read_header(fiberfile, ext=0)
    fahdr = fits.getheader(fiberfile, ext=0)
    targetdirs = [fahdr['TARG']]
    for moretarg in ['TARG2', 'TARG3', 'TARG4']:
        if moretarg in fahdr:
            if 'gaia' not in fahdr[moretarg]: # skip
                targetdirs += [fahdr[moretarg]]
    if 'SCND' in fahdr:
        if fahdr['SCND'].strip() != '-':
            targetdirs += [fahdr['SCND']]

    for ii, targetdir in enumerate(targetdirs):
        # for secondary targets, targetdir can be a filename
        if targetdir[-4:] == 'fits': # fragile...
            targetdir = os.path.dirname(targetdir)
        if not os.path.isdir(targetdir):
            # can be a KPNO directory!
            if 'DESIROOT' in targetdir:
                targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
            if targetdir[:6] == '/data/':
                targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
            
        if os.path.isdir(targetdir):
            log.debug('Found targets directory {}'.format(targetdir))
            targetdirs[ii] = targetdir
        else:
            log.warning('Targets directory {} not found.'.format(targetdir))
            continue

    # any ToOs?
    if 'TOO' in fahdr:
        TOO = photocache['TOO']
    else:
        TOO = None

    targetdirs = np.unique(np.hstack(targetdirs))
        
    return targetdirs, TOO

def read_target_photo(zcat, photocache, datamodel, targetdirs, TOO=None):
    """For a given tile, given a set of target directories and (optionally) a
    TOOfile used to build the set of targets, read and stack all the photometric
    targeting information for all objects.

    """
    # initialize the output catalog
    out = Table(np.hstack(np.repeat(datamodel, len(zcat))))
    out['TARGETID'] = zcat['TARGETID']

    targetdirs = np.unique(targetdirs)
    
    photo, photofiles = [], []
    for targetdir in targetdirs:
        # Handle secondary targets, which have a different data model;
        # update on 2021 July 31: these catalogs are missing DR9
        # photometry, so we have to skip them for now.
        if 'secondary' in targetdir:
            #continue                    
            if 'sv1' in targetdir: # special case
                if 'dedicated' in targetdir:
                    targetfiles = glob(os.path.join(targetdir, 'DC3R2_GAMA_priorities.fits'))
                else:
                    targetfiles = glob(os.path.join(targetdir, '*-secondary-dr9photometry.fits'))
            else:
                targetfiles = glob(os.path.join(targetdir, '*-secondary.fits'))
        else:
            alltargetfiles = glob(os.path.join(targetdir, '*-hp-*.fits'))
            filenside = fitsio.read_header(alltargetfiles[0], ext=1)['FILENSID']
            # https://github.com/desihub/desispec/issues/1711
            if np.any(np.isnan(zcat['TARGET_RA'])): # some SV1 targets have nan in RA,DEC
                log.warning('Some RA, DEC are NaN in target directory {}'.format(targetdir))
            notnan = np.isfinite(zcat['TARGET_RA'])
            
            targetfiles = []
            if np.sum(notnan) > 0:
                pixlist = radec2pix(filenside, zcat['TARGET_RA'][notnan], zcat['TARGET_DEC'][notnan])
                for pix in set(pixlist):
                    # /global/cfs/cdirs/desi/target/catalogs/gaiadr2/0.48.0/targets/sv1/resolve/supp/sv1targets-supp-hp-128.fits doesn't exist...
                    _targetfile = alltargetfiles[0].split('hp-')[0]+'hp-{}.fits'.format(pix)
                    if os.path.isfile(_targetfile):
                        targetfiles.append(_targetfile)

        targetfiles = np.unique(targetfiles)

        if len(targetfiles) == 0:
            continue

        #print(targetfiles)
        for ifile, targetfile in enumerate(targetfiles):
            # If this is a secondary target catalog, use the cache. Also note
            # that secondary target catalogs are missing some or all of the DR9
            # photometry columns we need, so only copy what exists, e.g.,
            #   /global/cfs/cdirs/desi/spectro/redux/everest/healpix/sv3/bright/153/15343/redrock-sv3-bright-15343.fits
            if targetfile in photocache.keys():
                if type(photocache[targetfile]) == astropy.table.Table:
                    I = np.where(np.isin(photocache[targetfile]['TARGETID'], zcat['TARGETID']))[0]
                else:
                    photo_targetid = photocache[targetfile]
                    I = np.where(np.isin(photo_targetid, zcat['TARGETID']))[0]
                    
                log.debug('Matched {} targets in {}'.format(len(I), targetfile))
                if len(I) > 0:
                    if type(photocache[targetfile]) == astropy.table.Table:
                        cachecat = photocache[targetfile][I]
                    else:
                        cachecat = Table(fitsio.read(targetfile, rows=I))
                    
                    _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
                    for col in _photo.colnames: # not all these columns will exist...
                        if col in cachecat.colnames:
                            _photo[col] = cachecat[col]
                    photofiles.append(targetfile)
                    photo.append(_photo)
                continue

            # get the correct extension name or number
            tinfo = fitsio.FITS(targetfile)
            for _tinfo in tinfo:
                extname = _tinfo.get_extname()
                if 'TARGETS' in extname:
                    break
            if extname == '':
                extname = 1
                
            # fitsio does not preserve the order of the rows but we'll sort later.
            photo_targetid = tinfo[extname].read(columns='TARGETID')
            I = np.where(np.isin(photo_targetid, zcat['TARGETID']))[0]
            #if len(I) == 0:
            #    log.warning('No matching targets!')
            #    raise ValueError
            
            log.debug('Matched {} targets in {}'.format(len(I), targetfile))
            if len(I) > 0:
                photo1 = tinfo[extname].read(rows=I)
                # Columns can be out of order, so sort them here based on the
                # data model so we can stack below.
                _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
                for col in _photo.colnames: # all these columns should exist...
                    if col in photo1.dtype.names:
                        _photo[col] = photo1[col]
                    else:
                        log.debug('Skipping missing column {} from {}'.format(col, targetfile))
                del photo1
                photofiles.append(targetfile)
                photo.append(_photo)

    # handle ToO targets
    if TOO is not None:
        I = np.where(np.isin(TOO['TARGETID'], zcat['TARGETID']))[0]
        log.debug('Matched {} TOO targets'.format(len(I)))
        if len(I) > 0:
            cachecat = TOO[I]
            _photo = Table(np.hstack(np.repeat(datamodel, len(I))))
            for col in _photo.colnames: # not all these columns will exist...
                if col in cachecat.colnames:
                    _photo[col] = cachecat[col]
            photofiles.append('TOO')
            photo.append(_photo)

    # backup programs have no target catalog photometry at all
    if len(photo) == 0:
        log.warning('No photometry found at all!')
        photo = [out] # empty set

    # np.hstack will sometimes complain even if the tables are identical...
    #photo = Table(np.hstack(photo))
    photo = vstack(photo)

    # make sure there are no duplicates...?
    _, uindx = np.unique(photo['TARGETID'], return_index=True) 
    photo = photo[uindx]
    assert(len(np.unique(photo['TARGETID'])) == len(photo))

    # sort explicitly in order to ensure order
    I = np.where(np.isin(out['TARGETID'], photo['TARGETID']))[0]
    srt = np.hstack([np.where(tid == photo['TARGETID'])[0] for tid in out['TARGETID'][I]])
    out[I] = photo[srt]
    
    return out

def main():

    p = argparse.ArgumentParser()
    p.add_argument('--reduxdir', type=str, help='spectro redux base dir overrides $DESI_SPECTRO_REDUX/$SPECPROD')
    p.add_argument('-o', '--outdir', type=str, required=True, help='output directory file')
    p.add_argument('--outprefix', type=str, required=True, help='output file prefix')
    p.add_argument('--mp', type=int, default=1, help='number of multiprocessing cores')
    p.add_argument('--filenside', type=int, default=4, help='healpix nside for tractorphot catalogs.')
    p.add_argument('--get-secondary-targetdirs', action='store_true', help='Figure out which secondary target catalogs actually were used to design tiles.')
    p.add_argument('--ntest', type=int, default=None, help='Test on a randomly selected sample of NTEST objects.')
    p.add_argument('--potential-targets', action='store_true', help='Gather photometry for the potential targets.')
    p.add_argument('--targetphot', action='store_true', help='Build the photo-targets catalogs.')
    p.add_argument('--tractorphot', action='store_true', help='Build the photo-tractor catalogs.')
    p.add_argument('--validate-tractorphot', action='store_true', help='Validate the photo-tractor catalogs.')
    p.add_argument('--overwrite', action='store_true', help='Overwrite existing photo-targets files.')
    
    args = p.parse_args()
    log = get_logger()

    if args.reduxdir is None:
        args.reduxdir = specprod_root()

    if not os.path.isdir(args.outdir):
        os.makedirs(args.outdir, exist_ok=True)
    if not os.path.isdir(os.path.join(args.outdir, 'tractorphot')):
        os.makedirs(os.path.join(args.outdir, 'tractorphot'), exist_ok=True)

    if args.ntest is not None:
        rand = np.random.RandomState(seed=1)
    else:
        rand = None

    if args.targetphot:
        if args.potential_targets:        
            outfile = os.path.join(args.outdir, 'targetphot-potential-targets-{}.fits'.format(args.outprefix))
        else:
            outfile = os.path.join(args.outdir, 'targetphot-{}.fits'.format(args.outprefix))
            
        if os.path.isfile(outfile) and not args.overwrite:
            log.info('Output file {} exists; use --overwrite'.format(outfile))
            return

        # Fragile! Read a single row of a single target catalog in order to get the
        # correct photometric data model. Add all the _TARGET columns at the end
        datamodel = Table(fitsio.read('/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/resolve/dark/targets-dark-hp-0.fits', rows=0, upper=True))
        for col in datamodel.colnames:
            if '_TARGET' in col:
                datamodel.remove_column(col)
            else:
                datamodel[col] = np.zeros(datamodel[col].shape, dtype=datamodel[col].dtype)
        for col in TARGETINGBITCOLS:
            datamodel[col] = np.zeros(1, dtype=np.int64)

        # This bit of code loops through all the fiberassign headers and figures
        # out which secondary catalogs are (ever) used for fiber assignment, so
        # we can cache them (below).
            
        if args.get_secondary_targetdirs:
            fiberfiles = np.hstack(glob(os.path.join(fiberassign_dir, '???', 'fiberassign-*.fits*')))
            #I = rand.choice(len(fiberfiles), size=50, replace=False) # for testing
            #fiberfiles = fiberfiles[I]
            targetdirs = []
            for ii, fiberfile in enumerate(fiberfiles):
                if ii % 500 == 0:
                    log.info('Working on fiberfile {}/{}'.format(ii, len(fiberfiles)))
                fahdr = fits.getheader(fiberfile, ext=0)        
                if 'SCND' in fahdr:
                    if fahdr['SCND'].strip() != '-':
                        targetdirs += [fahdr['SCND']]
            targetdirs = np.unique(np.hstack(targetdirs))
            final_targetdirs = []
            for ii, targetdir in enumerate(targetdirs):
                # can be a KPNO directory!
                if 'DESIROOT' in targetdir:
                    targetdir = os.path.join(desi_root, targetdir.replace('DESIROOT/', ''))
                if targetdir[:6] == '/data/':
                    targetdir = os.path.join(desi_root, targetdir.replace('/data/', ''))
                if os.path.isdir(targetdir):
                    targetdir = glob(os.path.join(targetdir, '*.fits'))
                for targetdir1 in np.atleast_1d(targetdir):                
                    if os.path.isfile(targetdir1):
                        log.info('Found secondary targets catalog {}'.format(targetdir1))
                        final_targetdirs.append(targetdir1)
                    else:
                        log.warning('Targets directory {} not found.'.format(targetdir))
            log.info(np.unique(final_targetdirs))
        
        # To speed things up, read and cache all the large secondary target catalogs and
        # the TOO files *once*. Now, not all these catalogs are actually used but we
        # don't know which ones yet until we read all the fiberassign headers.

        #cachefiles = np.hstack([
        #    glob('/global/cfs/cdirs/desi/target/catalogs/dr9/*/targets/*/secondary/*/*targets-*-secondary*.fits'),
        #    #glob('/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/*targets-*-secondary*.fits'),
        #    glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
        #    ])
    
        cachefiles = np.hstack([
            ['/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
             #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.48.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary-dr9photometry.fits',
             #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/bright/sv1targets-bright-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
             #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.50.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
             #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
             #'/global/cfs/cdirs/desi/target/catalogs/dr9/0.52.0/targets/sv1/secondary/dark/sv1targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/bright/targets-bright-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/1.0.0/targets/main/secondary/dark/targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/bright/targets-bright-secondary.fits',
             '/global/cfs/cdirs/desi/target/catalogs/dr9/1.1.1/targets/main/secondary/dark/targets-dark-secondary.fits',
             '/global/cfs/cdirs/desi/target/secondary/sv1/dedicated/0.49.0/DC3R2_GAMA_priorities.fits'],
            glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')
            ])
        
        #cachefiles = np.hstack([
        #        ['/global/cfs/cdirs/desi/target/catalogs/dr9/0.51.0/targets/sv1/secondary/dark/sv1targets-dark-secondary-dr9photometry.fits',
        #         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/bright/sv3targets-bright-secondary.fits',
        #         '/global/cfs/cdirs/desi/target/catalogs/dr9/0.57.0/targets/sv3/secondary/dark/sv3targets-dark-secondary.fits',
        #         ],
        #        glob('/global/cfs/cdirs/desi/survey/ops/surveyops/trunk/mtl/*/ToO/ToO.ecsv')])
        
        mpargs = []
        for cachefile in cachefiles:
            if cachefile.replace('.fits', '-dr9photometry.fits') in cachefiles and '.ecsv' not in cachefile: # just read the photometry
                continue
            mpargs.append([cachefile])
        if args.mp > 1:
            with multiprocessing.Pool(args.mp) as P:
                photocache1 = P.map(_cache_one_catalog, mpargs)
        else:
            photocache1 = [cache_one_catalog(mparg[0]) for mparg in mpargs]
        
        photocache = {}
        for _photocache1 in photocache1:
            for key in _photocache1.keys():
                if key in photocache.keys():
                    photocache[key] = vstack((_photocache1[key], photocache[key]), metadata_conflicts='silent') # stack the TOO catalogs
            photocache.update(_photocache1)

        if args.potential_targets:
            # Assume that the nominal/parent zcat catalog has been previously generated.
            parent_zcatfile = os.path.join(args.outdir, 'targetphot-zcat-{}.fits'.format(args.outprefix))
            if not os.path.isfile(parent_zcatfile):
                log.warning('Targeting photometric catalog {} missing'.format(parent_zcatfile))
                raise IOError
            log.info('Reading {}'.format(parent_zcatfile))
            parent_zcat = Table(fitsio.read(parent_zcatfile))

            if rand is not None:
                #parent_zcat = parent_zcat[parent_zcat['TILEID'] == 81108]
                J = rand.choice(len(parent_zcat), size=args.ntest, replace=False)
                parent_zcat = parent_zcat[J]

            tileids = np.unique(parent_zcat['TILEID'])

            bricks = None
            #bricks = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/survey-bricks.fits.gz'))
                        
            # Multiprocess over tiles
            log.info('Dividing the sample into {} unique tiles.'.format(len(tileids)))
            mpargs = [[tileid, bricks] for tileid in tileids]
                    
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_potential_targets, mpargs)
            else:
                zcat = [read_one_potential_targets(*mparg) for mparg in mpargs]

            # stack and find unique objects
            zcat = vstack(zcat)
            _, uindx = np.unique(zcat['TARGETID'], return_index=True)
            zcat = zcat[uindx]

            log.info('Found {:,} unique TARGETIDs from {:,} unique tiles.'.format(
                len(zcat), len(set(zcat['TILEID']))))
        else:
            # Read all the zpix catalogs in parallel.
            zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
            mpargs = []
            for zcatfile in zcatfiles:
                mpargs.append([zcatfile])
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    zcat = P.map(_read_one_zcat, mpargs)
            else:
                zcat = [read_one_zcat(mparg[0]) for mparg in mpargs]
    
            # Stack, remove negative targetids and sky fibers, and then select just
            # the unique TARGETIDs.
            zcat = vstack(zcat)
            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            #objid, brickid, release, mock, sky, gaia = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)
            if np.sum(keep) > 0:
                log.info('Keeping {}/{} objects after removing sky targets and stuck positioners)'.format(
                    np.sum(keep), len(zcat)))
                zcat = zcat[keep]
            
            _, uindx = np.unique(zcat['TARGETID'], return_index=True)
            zcat = zcat[uindx]
            zcat = zcat[np.argsort(zcat['TARGETID'])]
            
            log.info('Found {:,} unique TARGETIDs and {:,} unique tiles from {} redshift catalogs'.format(
                len(zcat), len(set(zcat['TILEID'])), len(zcatfiles)))
    
        if rand is not None:
            #zcat = zcat[zcat['TILEID'] == 81108]
            J = rand.choice(len(zcat), size=args.ntest, replace=False)
            zcat = zcat[J]

        if args.potential_targets:
            # Multiprocess over tiles
            log.info('Dividing the sample into {} unique tiles.'.format(len(set(zcat['TILEID']))))
            mpargs = [[zcat[zcat['TILEID'] == tileid], photocache, datamodel, args.reduxdir]
                      for tileid in set(zcat['TILEID'])]
        else:
            ## ~10^4 objects have NaN coordinates; handle those separately (all these have negative TARGETIDs)
            #inan = np.isnan(zcat['TARGET_RA'])
            #zcat_nan = zcat[inan]
            #zcat = zcat[np.logical_not(inan)]
    
            # Multiprocess over nside=8 healpixels which is how the non-secondary
            # targeting catalogs are organized on-disk.
            pixels = radec2pix(8, zcat['TARGET_RA'], zcat['TARGET_DEC'])
            log.info('Dividing the sample into {} nside=8 healpixels.'.format(len(set(pixels))))
            mpargs = [[zcat[pixel == pixels], photocache, datamodel, args.reduxdir]
                      for pixel in set(pixels)]
                    
        if args.mp > 1:
            with multiprocessing.Pool(args.mp) as P:
                out = P.map(_targetphot_one, mpargs)
        else:
            out = [targetphot_one(*mparg) for mparg in mpargs]

        # stack, sort, and write out
        out = vstack(out)
        out = out[np.argsort(out['TARGETID'])]

        out.meta['EXTNAME'] = 'TARGETPHOT'
        log.info('Writing {:,} objects to {}'.format(len(out), outfile))
        out.write(outfile, overwrite=True)
        del out, photocache

        # Also write out the redshift catalog (with a minimal number of columns)
        # used to generate the targetphot file. Note that this intermediate
        # catalog is needed in order to do positional Tractor photometry
        # gathering, below.
        outfile_zcat = outfile.replace('targetphot-', 'targetphot-zcat-')
        zcat.meta['EXTNAME'] = 'ZCATALOG'
        log.info('Writing {:,} objects to {}'.format(len(zcat), outfile_zcat))
        zcat.write(outfile_zcat, overwrite=True)
        del zcat
    
    # Build Tractor photometry.
    if args.tractorphot:
        if args.potential_targets:
            any_outfile = glob(os.path.join(args.outdir, 'tractorphot', 'tractorphot-potential-targets-nside{}-hp*-{}.fits'.format(
                args.filenside, args.outprefix)))
        else:
            any_outfile = glob(os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp*-{}.fits'.format(
                args.filenside, args.outprefix)))
        if len(any_outfile) > 0 and not args.overwrite:
            log.info('One or more tractorphot output files exist; use --overwrite')
            return

        if args.potential_targets:        
            zcatfile = os.path.join(args.outdir, 'targetphot-zcat-potential-targets-{}.fits'.format(args.outprefix))
            targetfile = os.path.join(args.outdir, 'targetphot-potential-targets-{}.fits'.format(args.outprefix))
        else:
            zcatfile = os.path.join(args.outdir, 'targetphot-zcat-{}.fits'.format(args.outprefix))
            targetfile = os.path.join(args.outdir, 'targetphot-{}.fits'.format(args.outprefix))
            
        if not os.path.isfile(targetfile):
            log.warning('Targeting photometric catalog {} missing'.format(targetfile))
            raise IOError

        if not os.path.isfile(zcatfile):
            log.warning('Targeting redshift catalog {} missing'.format(zcatfile))
            raise IOError

        if rand is not None:
            N = fitsio.FITS(targetfile)[1].get_nrows()
            J = rand.choice(N, size=args.ntest, replace=False)
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile, rows=J))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile, rows=J))
        else:
            #print('Hack!!!!!')
            #_tar = fitsio.read(targetfile, columns='TARGETID')
            #_junk = fitsio.read(os.path.join(args.outdir, 'tractorphot-validate-fuji.fits'), columns=['TARGETID', 'BRICKNAME_ZCAT'])
            #_junk = _junk[_junk['BRICKNAME_ZCAT'] == '']
            #_I = np.where(np.isin(_tar, _junk['TARGETID']))[0]
            #_tar = fitsio.read(targetfile, columns='TARGETID')
            #_I = np.where(np.isin(_tar, [39632961435338613, 39632966921487347, 39628509856927757]))
            # 39628509856927757  0352p315  503252        4109    9010 # brick_primary==False
            #_I = np.where(np.isin(_tar, [39628509856927757]))[0]
            #cat = Table(fitsio.read(targetfile, rows=_I))            
            log.info('Reading {}'.format(targetfile))
            cat = Table(fitsio.read(targetfile))
            log.info('Reading {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
        assert(np.all(cat['TARGETID'] == zcat['TARGETID']))
        
        # Create a mini-cat that we'll use for the searching.
        minicat = cat['TARGETID', 'PHOTSYS', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'RA', 'DEC']
        minicat['TARGET_RA'] = zcat['TARGET_RA']
        minicat['TARGET_DEC'] = zcat['TARGET_DEC']

        # Some secondary programs (e.g., 39632961435338613, 39632966921487347)
        # have BRICKNAME!='' & BRICKID!=0, but BRICK_OBJID==0. Fix those here
        # using decode_targetid.
        fix = np.where((minicat['BRICKNAME'] != '') * (minicat['BRICK_OBJID'] == 0))[0]
        if len(fix) > 0:
            log.info('Repairing BRICK_OBJID for {} objects using decode_targetid'.format(len(fix)))
            fix_objid, fix_brickid, _, _, _, _ = decode_targetid(minicat['TARGETID'][fix])
            assert(np.all(fix_brickid == minicat['BRICKID'][fix]))
            minicat['BRICK_OBJID'][fix] = fix_objid

        datamodel = Table(fitsio.read('/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/south/tractor/000/tractor-0001m002.fits', rows=0, upper=True))
        out = Table(np.hstack(np.repeat(datamodel, len(minicat))))
        for col in datamodel.colnames:
            out[col] = np.zeros(datamodel[col].shape, dtype=datamodel[col].dtype)
        out['TARGETID'] = minicat['TARGETID']
        
        # Infer brickname from the survey-bricks.fits file
        inobrickname = np.where(minicat['BRICKNAME'] == '')[0]
        if len(inobrickname) > 0:
            log.info('Inferring brickname for {:,} objects'.format(len(inobrickname)))

            if not os.path.isfile(bricksfile):
                log.warning('{} not found'.format(bricksfile))
                raise IOError
            log.info('Reading {}'.format(bricksfile))
            bricks = Table(fitsio.read(bricksfile))
            
            coord_bricks = SkyCoord(ra=bricks['RA']*u.deg, dec=bricks['DEC']*u.deg)
            coord_nobrickname = SkyCoord(ra=minicat['TARGET_RA'][inobrickname]*u.deg, dec=minicat['TARGET_DEC'][inobrickname]*u.deg)
            indx_bricks, d2d, _ = coord_nobrickname.match_to_catalog_sky(coord_bricks)
            minicat['BRICKNAME'][inobrickname] = bricks[indx_bricks]['BRICKNAME']
            
            #I = np.hstack([np.where(tid == zcat['TARGETID'])[0] for tid in minicat['TARGETID'][inobrickname]])
            #zcat_miss = zcat[I]
            #assert(np.all(cat_miss['TARGETID'] == zcat_miss['TARGETID']))

            ## old algorithm: get the brickname in parallel
            #mpargs = [[bricks, ra, dec] for ra, dec in zip(zcat_miss['TARGET_RA'], zcat_miss['TARGET_DEC'])]
            #if args.mp > 1:
            #    with multiprocessing.Pool(args.mp) as P:
            #        bricknames = P.map(_get_brickname, mpargs)
            #else:
            #    bricknames = [_get_brickname(mparg) for mparg in mpargs]
            #zcat_miss['BRICKNAME'] = np.hstack(bricknames)
        else:
            bricks = None
            
        assert(np.all(minicat['BRICKNAME'] != ''))

        # Loop over args.filenside healpixels and multiprocess over bricks. We
        # could/should MPI-parallelize over healpixels.
        log.info('Gathering Tractor photometry for {:,} objects'.format(len(minicat)))

        pixels = radec2pix(args.filenside, minicat['TARGET_RA'], minicat['TARGET_DEC'])
        npix = len(set(pixels))

        missing = []
        for ipix, pixel in enumerate(set(pixels)):
            log.info('Working on healpix {}/{} (nside={})'.format(ipix+1, npix, args.filenside))

            I = pixel == pixels
            log.info('Dividing the sample into {} unique bricks.'.format(len(set(minicat['BRICKNAME'][I]))))
            mpargs = []
            for brick in set(minicat['BRICKNAME'][I]):
                J = brick == minicat['BRICKNAME'][I]
                mpargs.append([minicat[I][J], bricks])
                
            if args.mp > 1:
                with multiprocessing.Pool(args.mp) as P:
                    tractor = P.map(_tractorphot_one, mpargs)
            else:
                tractor = [_tractorphot_one(mparg) for mparg in mpargs]
            tractor = vstack(tractor)

            # If there are any objects with missing Tractor photometry, keep track of them here.
            if len(tractor) == 0:
                imiss = np.ones(len(minicat[I][J]), bool)
            else:
                imiss = np.logical_not(np.isin(minicat['TARGETID'][I][J], tractor['TARGETID']))
                
            if np.sum(imiss) > 0:
                missing.append(cat[I][J][imiss])

            ## This can happen for healpix with just a couple targets; however,
            ## if it does it means we're going to miss targets when we do our
            ## positional matching, so raise an exception. Hopefully this will
            ## never happen but if it does then it means we need to keep track of
            ## these "missing" objects here.
            #if len(tractor) == 0 or len(tractor) != np.sum(I):
            #    log.warning('Some targets did not match on BRICKNAME and OBJID in healpix {}'.format(pixel))
            #    raise ValueError

            # Write out.
            if len(tractor) > 0:
                if args.potential_targets:
                    outfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-potential-targets-nside{}-hp{:03d}-{}.fits'.format(
                        args.filenside, pixel, args.outprefix))
                else:
                    outfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                        args.filenside, pixel, args.outprefix))
    
                tractor.meta['EXTNAME'] = 'TRACTORPHOT'
                tractor.meta['FILENSID'] = args.filenside
                log.info('Writing {:,} objects to {}'.format(len(tractor), outfile))
                tractor.write(outfile, overwrite=True)
            #del minicat

        if len(missing) > 0:
            missing = vstack(missing)
    
            if args.potential_targets:
                outfile_miss = os.path.join(args.outdir, 'targetphot-potential-targets-missing-{}.fits'.format(args.outprefix))
            else:
                outfile_miss = os.path.join(args.outdir, 'targetphot-missing-{}.fits'.format(args.outprefix))
            missing.meta['EXTNAME'] = 'TARGETPHOT'
            log.info('Writing {:,} objects to {}'.format(len(missing), outfile_miss))
            missing.write(outfile_miss, overwrite=True)
    
    # Validate the Tractor photometry against the original redshift catalogs.
    if args.validate_tractorphot:
        outcat = []
        outzcat = []
        
        zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-*-cumulative.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'zpix-sv3-dark.fits'))
        #zcatfiles = glob(os.path.join(args.reduxdir, 'zcatalog', 'ztile-sv1-other-cumulative.fits'))        
        for zcatfile in zcatfiles:
            log.info('Validating {}'.format(zcatfile))
            zcat = Table(fitsio.read(zcatfile))
            hdr = fitsio.read_header(zcatfile, ext=1)
            zcat['SURVEY'] = hdr['SURVEY']
            zcat['PROGRAM'] = hdr['PROGRAM']

            # Toss out skies and stuck positioners.
            _, _, _, _, sky, _ = decode_targetid(zcat['TARGETID'])
            keep = (sky == 0) * (zcat['TARGETID'] > 0)    
            zcat = zcat[keep]
        
            # read the appropriate Tractor catalog
            pixels = radec2pix(args.filenside, zcat['TARGET_RA'], zcat['TARGET_DEC'])
            log.info('Working on {} pixels'.format(len(set(pixels))))
            for pixel in set(pixels):
                I = pixel == pixels
                
                catfile = os.path.join(args.outdir, 'tractorphot', 'tractorphot-nside{}-hp{:03d}-{}.fits'.format(
                    args.filenside, pixel, args.outprefix))
                if not os.path.isfile(catfile):
                    log.info('No Tractor photometry for {} objects from {}'.format(np.sum(I), catfile))
                    # make sure they're all in the missing catalog
                    missfile = os.path.join(args.outdir, 'targetphot-missing-{}.fits'.format(args.outprefix))
                    miss = Table(fitsio.read(missfile))
                    assert(np.sum(np.isin(miss['TARGETID'], zcat['TARGETID'][I])) == np.sum(I))
                    continue
                    
                log.info('Reading {} objects from {}'.format(np.sum(I), catfile))
                cat = Table(fitsio.read(catfile))

                targetid = np.intersect1d(cat['TARGETID'], zcat['TARGETID'][I])
                cat = cat[np.isin(cat['TARGETID'], targetid)]
                _zcat = zcat[I][np.isin(zcat['TARGETID'][I], targetid)]
                J = [np.where(tid == cat['TARGETID'])[0] for tid in _zcat['TARGETID']]
                if len(J) == 0: # can happen when testing
                    continue
                J = np.hstack(J)
                cat = cat[J]
                assert(np.all(cat['TARGETID'] == _zcat['TARGETID']))

                diff = (cat['BRICKID'] != _zcat['BRICKID'])
                diff = np.logical_or(diff, cat['OBJID'] != _zcat['BRICK_OBJID'])
                #diff = np.logical_or(diff, cat['flux_g'] != _zcat['FLUX_R'])
                #diff = np.logical_or(diff, cat['flux_r'] != _zcat['FLUX_R'])
                #diff = np.logical_or(diff, cat['flux_z'] != _zcat['FLUX_Z'])
                #diff = np.logical_or(diff, cat['flux_w1'] != _zcat['FLUX_W1'])
                #diff = np.logical_or(diff, cat['flux_w2'] != _zcat['FLUX_W2'])
                diff = np.where(diff)[0]
        
                if np.sum(diff) > 0:
                    outcat.append(cat[diff])
                    outzcat.append(_zcat[diff])
                        
                    #objid, brickid, release, mock, sky, gaia = decode_targetid(_zcat['TARGETID'][diff])
                    #for dd in diff:
                    #    print(_zcat['BRICKID'][dd], cat['brickid'][dd], _zcat['BRICK_OBJID'][dd], cat['objid'][dd])
                    #print(set(release))

        if len(outcat) > 0:
            _outcat = vstack(outcat)
            _outzcat = vstack(outzcat)
            
            outcat = _outcat['TARGETID', 'RA', 'DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']
            outzcat = _outzcat['TARGETID', 'TARGET_RA', 'TARGET_DEC', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']

            for col in ['TARGETID', 'RELEASE', 'BRICKNAME', 'BRICKID', 'BRICK_OBJID', 'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2']:
                if col == 'BRICK_OBJID':
                    outzcat.rename_column(col, 'OBJID_ZCAT')
                else:
                    outzcat.rename_column(col, '{}_ZCAT'.format(col))
            #out = hstack((outcat, outzcat))

            # interlace the columns in the output table
            tractorcols = np.array(outcat.colnames)
            zcols = np.array(outzcat.colnames)
            out = Table()
            for ii in np.arange(len(zcols)):
                out[zcols[ii]] = outzcat[zcols[ii]]
                out[tractorcols[ii]] = outcat[tractorcols[ii]]

            # add brick_primary, survey, program, and the targeting bits
            out['BRICK_PRIMARY'] = _outcat['BRICK_PRIMARY']
            for col in _outzcat.colnames:
                if '_TARGET' in col:
                    out[col] = _outzcat[col]
            out['SURVEY'] = _outzcat['SURVEY']
            out['PROGRAM'] = _outzcat['PROGRAM']
    
            outfile = os.path.join(args.outdir, 'tractorphot-validate-{}.fits'.format(args.outprefix))
            out.write(outfile, overwrite=True)

if __name__ == '__main__':
    main()
