"""
desispec.bootcalib
==================

Utility functions to perform a quick calibration of DESI data

TODO:
1. Expand to r, i cameras
2. QA plots
3. Test with CR data
"""
from __future__ import print_function, absolute_import, division

import numpy as np
import copy
import pdb
import imp
import yaml
import glob
import math
import time
import os
import sys
import argparse
import locale
from pkg_resources import resource_exists, resource_filename

from astropy.modeling import models, fitting
from astropy.stats import sigma_clip
from astropy.table import Table, Column, vstack
from astropy.io import fits

from desispec.util import set_backend
set_backend()

from matplotlib import pyplot as plt
import matplotlib
import matplotlib.gridspec as gridspec
import matplotlib.cm as cm
from matplotlib.backends.backend_pdf import PdfPages

from desiutil.log import get_logger
from desiutil import funcfits as dufits
from numpy.polynomial.legendre import legval

glbl_figsz = (16,9)

########################################################
# High level wrapper
# TODO: This was ported from the original bin/desi_bootcalib so that it could
# be called independently by quicklook, but it needs to be coordinated with
# desispec.scripts.bootcalib.main()
########################################################

def bootcalib(deg,flatimage,arcimage):
    """
    Args:
        deg: Legendre polynomial degree to use to fit
        flatimage: desispec.image.Image object of flatfield
        arcimage: desispec.image.Image object of arc

    Mostly inherited from desispec/bin/desi_bootcalib directly as needed

    Returns:
        xfit, fdicts, gauss, all_wave_soln

    TODO: document what those return objects are
    """

    camera=flatimage.camera
    flat=flatimage.pix
    ny=flat.shape[0]

    xpk,ypos,cut=find_fiber_peaks(flat)
    xset,xerr=trace_crude_init(flat,xpk,ypos)
    xfit,fdicts=fit_traces(xset,xerr)
    gauss=fiber_gauss(flat,xfit,xerr)

    #- Also need wavelength solution not just trace

    arc=arcimage.pix
    arc_ivar=arcimage.ivar
    all_spec=extract_sngfibers_gaussianpsf(arc,arc_ivar,xfit,gauss)
    llist=load_arcline_list(camera)
    ### dlamb,wmark,gd_lines,line_guess=load_gdarc_lines(camera)
    dlamb, gd_lines = load_gdarc_lines(camera, llist)

    #- Solve for wavelengths
    all_wv_soln=[]
    all_dlamb=[]
    for ii in range(all_spec.shape[1]):
        spec=all_spec[:,ii]
        pixpk=find_arc_lines(spec)
        id_dict=id_arc_lines(pixpk,gd_lines,dlamb,wmark,line_guess=line_guess)
        id_dict['fiber']=ii
        #- Find the other good ones
        if camera == 'z':
            inpoly = 3  # The solution in the z-camera has greater curvature
        else:
            inpoly = 2
        add_gdarc_lines(id_dict, pixpk, gd_lines, inpoly=inpoly)
        #- Now the rest
        id_remainder(id_dict, pixpk, llist)
        #- Final fit wave vs. pix too
        final_fit, mask = dufits.iter_fit(np.array(id_dict['id_wave']), np.array(id_dict['id_pix']), 'polynomial', 3, xmin=0., xmax=1.)
        rms = np.sqrt(np.mean((dufits.func_val(np.array(id_dict['id_wave'])[mask==0],final_fit)-np.array(id_dict['id_pix'])[mask==0])**2))
        final_fit_pix,mask2 = dufits.iter_fit(np.array(id_dict['id_pix']), np.array(id_dict['id_wave']),'legendre',deg, niter=5)

        id_dict['final_fit'] = final_fit
        id_dict['rms'] = rms
        id_dict['final_fit_pix'] = final_fit_pix
        id_dict['wave_min'] = dufits.func_val(0,final_fit_pix)
        id_dict['wave_max'] = dufits.func_val(ny-1,final_fit_pix)
        id_dict['mask'] = mask
        all_wv_soln.append(id_dict)

    return xfit, fdicts, gauss,all_wv_soln


########################################################
# Arc/Wavelength Routines (Linelists come next)
########################################################

def find_arc_lines(spec,rms_thresh=7.,nwidth=5):
    """Find and centroid arc lines in an input spectrum

    Parameters
    ----------
    spec : ndarray
      Arc line spectrum
    rms_thresh : float
      RMS threshold scale
    nwidth : int
      Line width to test over
    """
    # Threshold criterion
    npix = spec.size
    spec_mask = sigma_clip(spec, sigma=4., iters=5)
    rms = np.std(spec_mask)
    thresh = rms*rms_thresh
    #print("thresh = {:g}".format(thresh))
    gdp = spec > thresh

    # Avoid edges
    gdp = gdp & (np.arange(npix) > 2.*nwidth) & (np.arange(npix) < (npix-2.*nwidth))

    # Roll to find peaks (simple algorithm)
    # nwidth = 5
    nstep = max(1,nwidth // 2)
    for kk in range(-nstep,nstep):
        if kk < 0:
            test = np.roll(spec,kk) < np.roll(spec,kk+1)
        else:
            test = np.roll(spec,kk) > np.roll(spec,kk+1)
        # Compare
        gdp = gdp & test

    # Center
    gdpix = np.where(gdp)[0]
    ngd = gdpix.size
    xpk = np.zeros(ngd)
    flux = np.zeros(ngd)

    for jj,igdpix in enumerate(gdpix):
        # Simple flux-weight
        pix = np.arange(igdpix-nstep,igdpix+nstep+1,dtype=int)
        flux[jj] =  np.sum(spec[pix])
        xpk[jj] = np.sum(pix*spec[pix]) / flux[jj]

    # Finish
    return xpk , flux



def remove_duplicates_w_id(wy,w,y_id,w_id) :
    # might be several identical w_id
    y_id=np.array(y_id).astype(int)
    w_id=np.array(w_id).astype(int)
    y_id2=[]
    w_id2=[]
    for j in np.unique(w_id) :
        w_id2.append(j)
        ii=y_id[w_id==j]
        if ii.size==1 :
            y_id2.append(ii[0])
        else :
            i=np.argmin(np.abs(wy[ii]-w[j]))
            y_id2.append(ii[i])
    y_id2=np.array(y_id2).astype(int)
    w_id2=np.array(w_id2).astype(int)
    tmp=np.argsort(w[w_id2])
    y_id2=y_id2[tmp]
    w_id2=w_id2[tmp]
    return y_id2,w_id2

def remove_duplicates_y_id(yw,y,y_id,w_id) :
    # might be several identical y_id
    w_id=np.array(w_id).astype(int)
    y_id=np.array(y_id).astype(int)
    w_id2=[]
    y_id2=[]
    for j in np.unique(y_id) :
        y_id2.append(j)
        ii=w_id[y_id==j]
        if ii.size==1 :
            w_id2.append(ii[0])
        else :
            i=np.argmin(np.abs(yw[ii]-y[j]))
            w_id2.append(ii[i])
    w_id2=np.array(w_id2).astype(int)
    y_id2=np.array(y_id2).astype(int)
    tmp=np.argsort(y[y_id2])
    w_id2=w_id2[tmp]
    y_id2=y_id2[tmp]
    return y_id2,w_id2


def refine_solution(y,w,y_id,w_id,deg=3,tolerance=5.) :

    log = get_logger()

    # remove duplicates
    transfo=np.poly1d(np.polyfit(y[y_id],w[w_id],deg=deg))
    wy=transfo(y)
    y_id,w_id=remove_duplicates_w_id(wy,w,y_id,w_id)
    transfo=np.poly1d(np.polyfit(w[w_id],y[y_id],deg=deg))
    yw=transfo(w)
    y_id,w_id=remove_duplicates_y_id(yw,y,y_id,w_id)

    if len(y_id) != len(np.unique(y_id)) :
        log.error("duplicate AT INIT y_id={:s}".format(str(y_id)))
    if len(w_id) != len(np.unique(w_id)) :
        log.error("duplicate AT INIT  w_id={:s}".format(str(w_id)))

    nmatch=len(y_id)
    #log.info("init nmatch=%d rms=%f wave=%s"%(nmatch,np.std(wy[y_id]-w[w_id]),w[w_id]))
    #log.info("init nmatch=%d rms=%f"%(nmatch,np.std(wy[y_id]-w[w_id])))
    if nmatch<deg+1 :
        log.error("error : init nmatch too small")
        return y_id,w_id,1000.,0

    rms=0.

    # loop on fit of transfo, pairing, cleaning
    for loop in range(200) :

        # compute transfo
        transfo=np.poly1d(np.polyfit(y[y_id],w[w_id],deg=deg))

        # apply transfo to measurements
        wy=transfo(y)

        previous_rms = rms+0.
        rms=np.std(wy[y_id]-w[w_id])

        # match lines
        mdiff0=min(tolerance,max(2.,rms*2.)) # this is a difficult parameter to tune, either loose lever arm, or have false matches !!
        mdiff1=tolerance # this is a difficult parameter to tune, either loose lever arm, or have false matches !!
        unmatched_indices=np.setdiff1d(np.arange(y.size),y_id)
        for i,wi in zip(unmatched_indices,wy[unmatched_indices]) :
            dist=np.abs(wi-w)
            jj=np.argsort(dist)
            for j,o in enumerate(jj) :
                if j in w_id :
                    continue
                if dist[j]<mdiff0 or ( o<jj.size-1 and dist[j]<mdiff1 and dist[j]<0.3*dist[jj[o+1]]) :
                    y_id=np.append(y_id,i)
                    w_id=np.append(w_id,j)
                    break

        previous_nmatch = nmatch+0
        nmatch=len(y_id)

        #log.info("iter #%d nmatch=%d rms=%f"%(loop,nmatch,rms))
        if nmatch < deg+1 :
           log.error("error init nmatch too small")
           y_id=[]
           w_id=[]
           rms=100000.
           return y_id,w_id,rms,loop

        if nmatch==previous_nmatch and abs(rms-previous_rms)<0.01 and loop>=1 :
            break
        if nmatch>=min(w.size,y.size) :
            #print("break because %d>=min(%d,%d)"%(nmatch,w.size,y.size))
            break

    return y_id,w_id,rms,loop

def id_remainder(id_dict, llist, deg=4, tolerance=1., verbose=False) :

    log = get_logger()

    y_id=np.array(id_dict['id_idx']).astype(int)
    all_y=np.array(id_dict['pixpk'])

    all_known_waves  = np.sort(np.array(llist["wave"]))
    identified_waves = np.array(id_dict["id_wave"]) # lines identified at previous steps

    w_id=[]
    for w in identified_waves :
        i=np.argmin(np.abs(all_known_waves-w))
        diff=np.abs(all_known_waves[i]-w)
        if diff>0.1 :
            log.warning("discrepant wavelength".format(w,all_known_waves[i]))
        w_id.append(i)
    w_id = np.array(w_id).astype(int)
    y_id,w_id,rms,niter=refine_solution(all_y,all_known_waves,y_id,w_id,deg=deg,tolerance=tolerance)

    id_dict['id_idx']  = np.sort(y_id)
    id_dict['id_pix']  = np.sort(all_y[y_id])
    id_dict['id_wave'] = np.sort(all_known_waves[w_id])
    id_dict['rms'] = rms

    log.info("{:d} matched for {:d} detected and {:d} known, rms = {:g}".format(len(y_id),len(all_y),len(all_known_waves),rms))


def compute_triplets(wave) :

    triplets=[]
    wave=np.sort(wave)
    for i1,w1 in enumerate(wave[:-1]) :
        for i2,w2 in enumerate(wave[i1+1:]) :
            for i3,w3 in enumerate(wave[i1+i2+2:]) :
                triplet=[w1,w2,w3,i1,i1+1+i2,i1+i2+2+i3,w2-w1,w3-w1,w2**2-w1**2,w3**2-w1**2]
                #print(triplet)
                #print(wave[i1],wave[i1+1+i2],wave[i1+i2+2+i3])
                triplets.append(triplet)
    return np.array(triplets)

def id_arc_lines_using_triplets(id_dict,w,dwdy_prior,d2wdy2_prior=1.5e-5,toler=0.2,ntrack=50,nmax=40):
    """Match (as best possible), a set of the input list of expected arc lines to the detected list

    Parameters
    ----------
    id_dict : dictionnary with Pixel locations of detected arc lines in "pixpk" and fluxes in "flux"
    w : ndarray
      array of expected arc lines to be detected and identified
    dwdy : float
      Average dispersion in the spectrum
    d2wdy2_prior : float
      Prior on second derivative
    toler : float, optional
      Tolerance for matching (20%)
    ntrack : max. number of solutions to be tracked

    Returns
    -------
    id_dict : dict
      dict of identified lines
    """

    log=get_logger()
    #log.info("y=%s"%str(y))
    #log.info("w=%s"%str(w))


    y = id_dict["pixpk"]

    log.info("ny=%d nw=%d"%(len(y),len(w)))

    if nmax<10 :
        nmax=10
        log.warning("force nmax=10 (arg was too small: {:d})".format(nmax))

    if len(y)>nmax :
        # log.info("down-selecting the number of detected lines from {:d} to {:d}".format(len(y),nmax))
        # keep at least the edges
        margin=3
        new_y=np.append(y[:margin],y[-margin:])
        # now look at the flux to select the other ones
        flux=id_dict["flux"][margin:-margin]
        ii=np.argsort(flux)
        new_y=np.append(new_y,y[margin:-margin][ii[-(nmax-2*margin):]])
        y = np.sort(new_y)

    # compute triplets of waves of y positions
    y_triplets = compute_triplets(y)
    w_triplets = compute_triplets(w)

    # each pair of triplet defines a 2nd order polynomial (chosen centered on y=2000)
    # w = a*(y-2000)**2+b*(y-2000)+c
    # w = a*y**2-4000*a*y+b*y+cst
    # w = a*(y**2-4000*y)+b*y+cst
    # dw_12 = a*(dy2_12-4000*dy_12)+b*dy_12
    # dw_13 = a*(dy2_13-4000*dy_13)+b*dy_12
    # dw_12 = a*cdy2_12+b*dy_12
    # dw_13 = a*cdy2_13+b*dy_13
    # with cdy2_12=dy2_12-4000*dy_12
    # and  cdy2_13=dy2_13-4000*dy_13
    # idet = 1./(dy_13*cdy2_12-dy_12*cdy2_13)
    # a = idet*(dy_13*dw_12-dy_12*dw_13)
    # b = idet*(-cdy2_13*dw_12+cdy2_12*dw_13)

    #triplet=[w1,w2,w3,i1,i1+1+i2,i1+i2+2+i3,w2-w1,w3-w1,w2**2-w1**2,w3**2-w1**2]
    dy_12=y_triplets[:,6]
    dy_13=y_triplets[:,7]
    #dy2_12=y_triplets[:,8]
    #dy2_13=y_triplets[:,9]
    # centered version
    cdy2_12=y_triplets[:,8]-4000.*y_triplets[:,6]
    cdy2_13=y_triplets[:,9]-4000.*y_triplets[:,7]
    idet=1./(dy_13*cdy2_12-dy_12*cdy2_13)

    # fill histogram with polynomial coefs and first index of each triplet in the pair for all pairs of triplets(y,w)
    # create the 4D histogram
    ndwdy    = 41
    nd2wdy2  = 21
    dwdy_min = dwdy_prior*(1-toler)
    dwdy_max = dwdy_prior*(1+toler)
    dwdy_step = (dwdy_max-dwdy_min)/ndwdy
    d2wdy2_min = -d2wdy2_prior
    d2wdy2_max = +d2wdy2_prior
    d2wdy2_step = (d2wdy2_max-d2wdy2_min)/nd2wdy2
    histogram = np.zeros((ndwdy,nd2wdy2,len(y),len(w))) # definition of the histogram

    # fill the histogram
    for w_triplet in w_triplets :
        #d2wdy2 = idet*(dy_13*w_triplet[6]-dy_12*w_triplet[7])
        #dwdy   = idet*(-cdy2_13*w_triplet[6]+cdy2_12*w_triplet[7])
        # bins in the histogram
        dwdy_bin   = ((idet*(-cdy2_13*w_triplet[6]+cdy2_12*w_triplet[7])-dwdy_min)/dwdy_step).astype(int)
        d2wdy2_bin = ((idet*(dy_13*w_triplet[6]-dy_12*w_triplet[7])-d2wdy2_min)/d2wdy2_step).astype(int)
        pairs_in_histo=np.where((dwdy_bin>=0)&(dwdy_bin<ndwdy)&(d2wdy2_bin>=0)&(d2wdy2_bin<nd2wdy2))[0]
        # fill histo
        iw=int(w_triplet[3])
        for a,b,c in zip(dwdy_bin[pairs_in_histo],d2wdy2_bin[pairs_in_histo],y_triplets[pairs_in_histo,3].astype(int)) :
            histogram[a,b,c,iw] += 1

    # find max bins in the histo
    histogram_ravel = histogram.ravel()
    best_histo_bins = histogram_ravel.argsort()[::-1]
    #log.info("nmatch in first bins=%s"%histogram.ravel()[best_histo_bins[:3]])

    best_y_id=[]
    best_w_id=[]
    best_rms=1000.

    # loop on best matches ( = most populated bins)
    count=0
    for histo_bin in best_histo_bins[:ntrack] :

        if  histogram_ravel[histo_bin]<4 and count>3 :
            log.warning("stopping here")
            break
        count += 1
        dwdy_best_bin,d2wdy2_best_bin,iy_best_bin,iw_best_bin = np.unravel_index(histo_bin, histogram.shape) # bin coord
        #print("bins=",dwdy_best_bin,d2wdy2_best_bin,iy_best_bin,iw_best_bin)

        # pairs of triplets in this histo bin
        w_id=np.array([])
        y_id=np.array([])
        wok=np.where(w_triplets[:,3]==iw_best_bin)[0]
        yok=np.where(y_triplets[:,3]==iy_best_bin)[0]
        for w_triplet in w_triplets[wok] :
            #d2wdy2 = idet[yok]*(dy_13[yok]*w_triplet[6]-dy_12[yok]*w_triplet[7])
            #dwdy   = idet[yok]*(-cdy2_13[yok]*w_triplet[6]+cdy2_12[yok]*w_triplet[7])
            # bins in the histogram
            dwdy_bin   = ((idet[yok]*(-cdy2_13[yok]*w_triplet[6]+cdy2_12[yok]*w_triplet[7])-dwdy_min)/dwdy_step).astype(int)
            d2wdy2_bin = ((idet[yok]*(dy_13[yok]*w_triplet[6]-dy_12[yok]*w_triplet[7])-d2wdy2_min)/d2wdy2_step).astype(int)
            wyok=yok[np.where((dwdy_bin==dwdy_best_bin)&(d2wdy2_bin==d2wdy2_best_bin))[0]]
            for y_triplet in y_triplets[wyok] :
                y_id=np.append(y_id,y_triplet[3:6])
                w_id=np.append(w_id,w_triplet[3:6])

        # now need to rm duplicates
        nw=len(w)
        ny=len(y)
        unique_common_id=np.unique(y_id.astype(int)*nw+w_id.astype(int))
        y_id=(unique_common_id/nw).astype(int)
        w_id=(unique_common_id%nw).astype(int)
        ordering=np.argsort(y[y_id])
        y_id=y_id[ordering]
        w_id=w_id[ordering]
        # refine
        y_id,w_id,rms,niter=refine_solution(y,w,y_id,w_id)
        #log.info("get solution with %d match and rms=%f (niter=%d)"%(len(y_id),rms,niter))
        if (len(y_id)>len(best_y_id) and rms<max(1,best_rms)) or (len(y_id)==len(best_y_id) and rms<best_rms) or (best_rms>1 and rms<1 and len(y_id)>=8) :
            #log.info("new best solution #%d with %d match and rms=%f (niter=%d)"%(count,len(y_id),rms,niter))
            #log.info("previous had %d match and rms=%f"%(len(best_y_id),best_rms))
            best_y_id = y_id
            best_w_id = w_id
            best_rms = rms

        # stop at some moment
        if best_rms<0.2 and len(y_id)>=min(15,min(len(y),len(w))) :
            #log.info("stop here because we have a correct solution")
            break

    if len(y) != len(id_dict["pixpk"]) :
        #log.info("re-indexing the result")
        tmp_y_id = []
        for i in best_y_id :
            tmp_y_id.append(np.argmin(np.abs(id_dict["pixpk"]-y[i])))
        best_y_id = np.array(tmp_y_id).astype(int)
        y = id_dict["pixpk"]

    if len(best_w_id) == 0 :
        log.error("failed, no match")
        id_dict["status"]="failed"
        id_dict["id_idx"]=[]
        id_dict["id_pix"]=[]
        id_dict["id_wave"]=[]
        id_dict["rms"]=999.
        id_dict["fit"]=None
        return

    id_dict["status"]="ok"
    id_dict["id_idx"]=best_y_id
    id_dict["id_pix"]=y[best_y_id]
    id_dict["id_wave"]=w[best_w_id]
    id_dict["rms"]=best_rms
    deg=max(1,min(3,best_y_id.size-2))
    id_dict["fit"]= dufits.func_fit(w[best_w_id],y[best_y_id],'polynomial',deg,xmin=0.,xmax=1.)

    log.info("{:d} matched for {:d} detected and {:d} known as good, rms = {:g}".format(len(best_y_id),len(y),len(w),best_rms))



########################################################
# Linelist routines
########################################################


def parse_nist(ion, vacuum=True):
    """Parse a NIST ASCII table.

    Note that the long ---- should have
    been commented out and also the few lines at the start.

    Taken from PYPIT

    Parameters
    ----------
    ion : str
      Name of ion
    vaccuum : bool, optional
      Use vacuum wavelengths
    """
    log=get_logger()
    # Find file
    medium = 'vacuum'
    if not vacuum:
        log.info("Using air wavelengths")
        medium = 'air'
    srch_file = "data/arc_lines/{0}_{1}.ascii".format(ion, medium)
    if not resource_exists('desispec', srch_file):
        log.error("Cannot find NIST file {:s}".format(srch_file))
        raise Exception("Cannot find NIST file {:s}".format(srch_file))
    # Read, while working around non-ASCII characters in NIST line lists
    nist_file = resource_filename('desispec', srch_file)
    log.info("reading NIST file {:s}".format(nist_file))
    default_locale = locale.getlocale(locale.LC_CTYPE)
    locale.setlocale(locale.LC_CTYPE, 'en_US.UTF-8')
    nist_tbl = Table.read(nist_file, format='ascii.fixed_width')
    locale.setlocale(locale.LC_CTYPE, default_locale)
    gdrow = nist_tbl['Observed'] > 0.  # Eliminate dummy lines
    nist_tbl = nist_tbl[gdrow]
    # Now unique values only (no duplicates)
    uniq, indices = np.unique(nist_tbl['Observed'],return_index=True)
    nist_tbl = nist_tbl[indices]
    # Deal with Rel
    agdrel = []
    for row in nist_tbl:
        try:
            gdrel = int(row['Rel.'])
        except:
            try:
                gdrel = int(row['Rel.'][:-1])
            except:
                gdrel = 0
        agdrel.append(gdrel)
    agdrel = np.array(agdrel)
    # Remove and add
    nist_tbl.remove_column('Rel.')
    nist_tbl.remove_column('Ritz')
    nist_tbl.add_column(Column(agdrel,name='RelInt'))
    nist_tbl.add_column(Column([ion]*len(nist_tbl), name='Ion', dtype=(str, 5)))
    nist_tbl.rename_column('Observed','wave')
    # Return
    return nist_tbl


def load_arcline_list(camera, vacuum=True,lamps=None):

    """Loads arc line list from NIST files
    Parses and rejects

    Taken from PYPIT

    Parameters
    ----------
    lines : list
      List of ions to load
    vacuum : bool, optional
      Use vacuum wavelengths
    lamps : optional numpy array of ions, ex np.array(["HgI","CdI","ArI","NeI"])

    Returns
    -------
    alist : Table
      Table of arc lines
    """
    log=get_logger()
    wvmnx = None
    if lamps is None :
        if camera[0] == 'b':
            lamps = ['CdI','ArI','HgI','NeI','KrI']
        elif camera[0] == 'r':
            lamps = ['CdI','ArI','HgI','NeI','KrI']
        elif camera[0] == 'z':
            lamps = ['CdI','ArI','HgI','NeI','KrI']
        elif camera == 'all': # Used for specex
            lamps = ['CdI','ArI','HgI','NeI','KrI']
        else:
            log.error("Not ready for this camera")

    # Get the parse dict
    parse_dict = load_parse_dict()
    # Read rejection file
    medium = 'vacuum'
    if not vacuum:
        log.info("Using air wavelengths")
        medium = 'air'
    rej_file = resource_filename('desispec', "data/arc_lines/rejected_lines_{0}.yaml".format(medium))
    with open(rej_file, 'r') as infile:
        rej_dict = yaml.load(infile)
    # Loop through the NIST Tables
    tbls = []
    for iline in lamps:
        # Load
        tbl = parse_nist(iline, vacuum=vacuum)
        # Parse
        if iline in parse_dict:
            tbl = parse_nist_tbl(tbl,parse_dict[iline])
        # Reject
        if iline in rej_dict:
            log.info("Rejecting select {:s} lines".format(iline))
            tbl = reject_lines(tbl,rej_dict[iline])
        #print("DEBUG",iline)
        #print("DEBUG",tbl[['Ion','wave','RelInt']])
        tbls.append(tbl[['Ion','wave','RelInt']])
    # Stack
    alist = vstack(tbls)

    # wvmnx?
    if wvmnx is not None:
        print('Cutting down line list by wvmnx: {:g},{:g}'.format(wvmnx[0],wvmnx[1]))
        gdwv = (alist['wave'] >= wvmnx[0]) & (alist['wave'] <= wvmnx[1])
        alist = alist[gdwv]
    # Return
    return alist


def reject_lines(tbl,rej_dict, rej_tol=0.1):
    """Rejects lines from a NIST table

    Taken from PYPIT

    Parameters
    ----------
    tbl : Table
      Read previously from NIST ASCII file
    rej_dict : dict
      Dict of rejected lines
    rej_tol : float, optional
      Tolerance for matching a line to reject to linelist (Angstroms)

    Returns
    -------
    tbl : Table
      Rows not rejected
    """
    msk = tbl['wave'] == tbl['wave']
    # Loop on rejected lines
    for wave in rej_dict:
        close = np.where(np.abs(wave-tbl['wave']) < rej_tol)[0]
        if rej_dict[wave] == 'all':
            msk[close] = False
        else:
            raise ValueError('Not ready for this')
    # Return
    return tbl[msk]

def parse_nist_tbl(tbl,parse_dict):
    """Parses a NIST table using various criteria

    Parameters
    ----------
    tbl : Table
      Read previously from NIST ASCII file
    parse_dict : dict
      Dict of parsing criteria.  Read from load_parse_dict

    Returns
    -------
    tbl : Table
      Rows meeting the criteria
    """
    # Parse
    gdI = tbl['RelInt'] >= parse_dict['min_intensity']
    gdA = tbl['Aki'] >= parse_dict['min_Aki']
    gdw = tbl['wave'] >= parse_dict['min_wave']
    # Combine
    allgd = gdI & gdA & gdw
    # Return
    return tbl[allgd]

def load_parse_dict():
    """Dicts for parsing Arc line lists from NIST

    Rejected lines are in the rejected_lines.yaml file
    """
    dict_parse = dict(min_intensity=0., min_Aki=0., min_wave=0.)
    arcline_parse = {}
    # ArI
    arcline_parse['ArI'] = copy.deepcopy(dict_parse)
    arcline_parse['ArI']['min_intensity'] = 1000. # NOT PICKING UP REDDEST LINES
    # HgI
    arcline_parse['HgI'] = copy.deepcopy(dict_parse)
    arcline_parse['HgI']['min_intensity'] = 800.
    # HeI
    arcline_parse['HeI'] = copy.deepcopy(dict_parse)
    arcline_parse['HeI']['min_intensity'] = 20.
    # NeI
    arcline_parse['NeI'] = copy.deepcopy(dict_parse)
    arcline_parse['NeI']['min_intensity'] = 999.
    #arcline_parse['NeI']['min_Aki']  = 1. # NOT GOOD FOR DEIMOS, DESI
    #arcline_parse['NeI']['min_wave'] = 5700.
    arcline_parse['NeI']['min_wave'] = 5850. # NOT GOOD FOR DEIMOS?
    # ZnI
    arcline_parse['ZnI'] = copy.deepcopy(dict_parse)
    arcline_parse['ZnI']['min_intensity'] = 50.
    # KrI
    arcline_parse['KrI'] = copy.deepcopy(dict_parse)
    arcline_parse['KrI']['min_intensity'] = 50.
    return arcline_parse


def load_gdarc_lines(camera, llist, vacuum=True,lamps=None,good_lines_filename=None):

    """Loads a select set of arc lines for initial calibrating

    Parameters
    ----------
    camera : str
      Camera ('b', 'g', 'r')
    llist : table of lines to use, with columns Ion, wave
    vacuum : bool, optional
      Use vacuum wavelengths
    lamps : optional numpy array of ions, ex np.array(["HgI","CdI","ArI","NeI"])

    Returns
    -------
    dlamb : float
      Dispersion for input camera
    wmark : float
      wavelength to key off of [???]
    gd_lines : ndarray
      Array of lines expected to be recorded and good for ID
    line_guess : int or None
      Guess at the line index corresponding to wmark (default is to guess the 1/2 way point)
    """
    log=get_logger()

    if lamps is None :
        lamps=np.array(["HgI","CdI","ArI","NeI"])

    lines={}

    dlamb=0.6
    if camera[0] == 'b':
        dlamb = 0.589
    elif camera[0] == 'r':
        dlamb = 0.527
    elif camera[0] == 'z':
        #dlamb = 0.599  # Ang
        dlamb = 0.608  # Ang (from teststand, ranges (fiber & wave) from 0.54 to 0.66)
    # read good lines
    if good_lines_filename is not None :
        filename = good_lines_filename
    else :
        if vacuum :
            filename = resource_filename('desispec', "data/arc_lines/goodlines_vacuum.ascii")
        else :
            filename = resource_filename('desispec', "data/arc_lines/goodlines_air.ascii")

    log.info("Reading good lines in {:s}".format(filename))
    lines={}
    ifile=open(filename)
    for line in ifile.readlines() :
        if line[0]=="#" :
            continue
        vals=line.strip().split()
        if len(vals)<3 :
            log.warning("ignoring line '{:s}' in {:s}".format(line.strip(),filename))
            continue
        cameras=vals[2]
        if cameras.find(camera[0].upper()) < 0 :
            continue
        ion=vals[1]
        wave=float(vals[0])
        if ion in lines:
            lines[ion].append(wave)
        else :
            lines[ion]=[wave,]
    ifile.close()
    log.info("Good lines = {:s}".format(str(lines)))

    log.info("Checking consistency with full line list")
    nbad=0
    for ion in lines:
        ii=np.where(llist["Ion"]==ion)[0]
        if ii.size == 0 :
            continue
        all_waves=np.array(llist["wave"][ii])
        for j,w in enumerate(lines[ion]) :
            i=np.argmin(np.abs(w-all_waves))
            if np.abs(w-all_waves[i])>0.2 :
                log.error("cannot find good line {:f} of {:s} in full line list. nearest is {:f}".format(w,ion,all_waves[i]))
                nbad += 1
            elif np.abs(w-all_waves[i])>0.001 :
                log.warning("adjusting hardcoded {:s} line {:f} -> {:f} (the NIST line list is the truth)".format(w,ion,all_waves[i]))
                lines[ion][j]=all_waves[i]
    if nbad>0 :
        log.error("{:d} inconsistent hardcoded lines, exiting".format(nbad))
        sys.exit(12)

    gd_lines=np.array([])
    for lamp in lamps :
        if lamp in lines:
            gd_lines=np.append(gd_lines,lines[lamp])

    # Sort and return
    gd_lines.sort()
    return dlamb, gd_lines

########################################################
# Fiber routines
########################################################

def fiber_gauss(flat, xtrc, xerr, box_radius=2, max_iter=5, debug=False, verbose=False) :
    return fiber_gauss_new(flat, xtrc, xerr, box_radius, max_iter)

def fiber_gauss_new(flat, xtrc, xerr, box_radius=2, max_iter=5, debug=False, verbose=False):
    """Find the PSF sigma for each fiber
    This serves as an initial guess to what follows

    Parameters
    ----------
    flat : ndarray of fiber flat image
    xtrc: ndarray of fiber traces
    xerr: ndarray of error in fiber traces
    box_radius: int, optinal
          Radius of boxcar extraction in pixels
    max_iter : int, optional
      Maximum number of iterations for rejection

    Returns
    -------
    gauss
        list of Gaussian sigma
    """
    log=get_logger()

    npix_y  = flat.shape[0]
    npix_x  = flat.shape[1]
    ny      = xtrc.shape[0] # number of ccd rows in trace
    assert(ny==npix_y)

    nfiber = xtrc.shape[1]

    minflux=1. # minimal flux in a row to include in the fit

    # Loop on fibers
    gauss = []
    start = 0
    for ii in range(nfiber):
        if (ii % 25 == 0): # & verbose:
            stop=time.time()
            if start==0 :
                log.info("Working on fiber {:d} of {:d}".format(ii,nfiber))
            else :
                log.info("Working on fiber %d of %d (25 done in %3.2f sec)"%(ii,nfiber,stop-start))
            start=stop

        # collect data
        central_xpix=np.floor(xtrc[:,ii]+0.5)
        begin_xpix=(central_xpix-box_radius).astype(int)
        end_xpix=(central_xpix+box_radius+1).astype(int)
        dx=[]
        flux=[]
        for y in range(ny) :
            yflux=flat[y,begin_xpix[y]:end_xpix[y]]
            syflux=np.sum(yflux)
            if syflux<minflux :
                continue
            dx.append(np.arange(begin_xpix[y],end_xpix[y])-(xtrc[y,ii]))
            flux.append(yflux/syflux)
        dx=np.array(dx)
        flux=np.array(flux)

        # compute profile
        # one way to get something robust is to compute median in bins
        # it's a bit biasing but the PSF is not a Gaussian anyway
        bins=np.linspace(-box_radius,box_radius,100)
        bstep=bins[1]-bins[0]
        bdx=[]
        bflux=[]
        for b in bins :
            ok=(dx>=b)&(dx<(b+bstep))
            if np.sum(ok)>0 :
                bdx.append(np.mean(dx[ok]))
                bflux.append(np.median(flux[ok]))
        if len(bdx)<10 :
            log.error("sigma fit failed for fiber #%02d"%ii)
            log.error("this should only occur for the fiber near the center of the detector (if at all)")
            log.error("using the sigma value from the previous fiber")
            gauss.append(gauss[-1])
            continue
        # this is the profile :
        bdx=np.array(bdx)
        bflux=np.array(bflux)

        # fast iterative gaussian fit
        sigma = 1.0
        sq2 = math.sqrt(2.)
        for i in range(10) :
            nsigma = sq2*np.sqrt(np.mean(bdx**2*bflux*np.exp(-bdx**2/2/sigma**2))/np.mean(bflux*np.exp(-bdx**2/2/sigma**2)))
            if abs(nsigma-sigma) < 0.001 :
                break
            sigma = nsigma
        gauss.append(sigma)

    return np.array(gauss)



def fiber_gauss_old(flat, xtrc, xerr, box_radius=2, max_iter=5, debug=False, verbose=False):
    """Find the PSF sigma for each fiber
    This serves as an initial guess to what follows

    Parameters
    ----------
    flat : ndarray of fiber flat image
    xtrc: ndarray of fiber traces
    xerr: ndarray of error in fiber traces
    box_radius: int, optinal
        Radius of boxcar extraction in pixels
    max_iter : int, optional
        Maximum number of iterations for rejection

    Returns
    -------
    gauss
        list of Gaussian sigma
    """
    log=get_logger()
    log.warning("fiber_gauss uses astropy.modeling.  Consider an alternative")
    # Init
    nfiber = xtrc.shape[1]
    ny = xtrc.shape[0]
    iy = np.arange(ny).astype(int)
    # Mask
    mask = np.zeros_like(flat,dtype=int)
    # Sub images
    xpix_img = np.outer(np.ones(flat.shape[0]),np.arange(flat.shape[1]))
    # Gaussian fit
    g_init = models.Gaussian1D(amplitude=1., mean=0., stddev=1.)
    g_init.amplitude.fixed = True
    g_init.mean.fixed = True
    fitter = fitting.LevMarLSQFitter()

    # Loop on fibers
    gauss = []
    start = 0
    for ii in range(nfiber):
        if (ii % 25 == 0): # & verbose:
            stop=time.time()
            if start==0 :
                log.info("Working on fiber {:d} of {:d}".format(ii,nfiber))
            else :
                log.info("Working on fiber %d of %d (done 25 in %3.2f sec)"%(ii,nfiber,stop-start))
            start=stop

        mask[:] = 0
        ixt = np.round(xtrc[:,ii]).astype(int)
        for jj,ibox in enumerate(range(-box_radius,box_radius+1)):
            ix = ixt + ibox
            mask[iy,ix] = 1
        dx_img = xpix_img - np.outer(xtrc[:,ii],np.ones(flat.shape[1]))
        # Sum
        flux = np.sum(mask*flat,axis=1)
        flux = np.maximum(flux,1.)
        # Normalize
        nrm_img = flat / np.outer(flux,np.ones(flat.shape[1]))
        # Gaussian
        cpix = np.where(np.abs(dx_img)<0.10)
        if len(cpix[0]) < 50:
            cpix = np.where(np.abs(dx_img)<0.40)
        amp = np.median(nrm_img[cpix])
        g_init.amplitude.value = amp # Fixed
        fdimg = dx_img[mask==1].flatten()
        fnimg = nrm_img[mask==1].flatten()
        # Guess at sigma
        gdfn = (fnimg < amp) & (fnimg > 0.)
        all_sig = np.abs(fdimg[gdfn]) / np.sqrt( np.log(amp)-np.log(fnimg[gdfn]) )
        g_init.stddev.value = np.median(all_sig[np.where((np.abs(fdimg[gdfn])>1) & (np.abs(fdimg[gdfn])<1.5) & (np.isfinite(all_sig)))])

        # Initial fit (need to mask!)
        parm = fitter(g_init, fdimg, fnimg)

        # Iterate
        iterate = True
        nrej = 0
        niter = 0
        while iterate & (niter < max_iter):
            # Clip
            resid = parm(fdimg) - fnimg
            resid_mask = sigma_clip(resid, sigma=4., iters=5)
            # Fit
            gdp = ~resid_mask.mask
            parm = fitter(g_init, fdimg[gdp], fnimg[gdp])
            # Again?
            if np.sum(resid_mask.mask) <= nrej:
                iterate = False
            else:
                nrej = np.sum(resid_mask.mask)
                niter += 1
        if verbose:
            log.info("Rejected {:d} in {:d} iterations".format(nrej,niter))

        #debug = False
        if debug:
            plt.clf()
            plt.scatter(fdimg[gdp], fnimg[gdp])
            x= np.linspace(-box_radius, box_radius, 200)
            plt.plot(x, parm(x), 'r-')
            plt.show()
            plt.close()
            pdb.set_trace()
        # Save
        gauss.append(parm.stddev.value)
    #
    return np.array(gauss)

def find_fiber_peaks(flat, ypos=None, nwidth=5, debug=False) :
    """Find the peaks of the fiber flat spectra
    Preforms book-keeping error checking

    Args:
        flat : ndarray of fiber flat image
        ypos : int [optional] Row for finding peaks
           Default is half-way up the image
        nwidth : int [optional] Width of peak (end-to-end)
        debug: bool, optional

    Returns:
        xpk, ypos, cut
            list of xpk (nearest pixel) at ypos
            ndarray of cut through the image
    """
    log=get_logger()
    log.info("starting")
    # Init
    Nbundle = 20
    Nfiber = 25 # Fibers per bundle
    # Set ypos for peak finding
    if ypos is None:
        ypos = flat.shape[0]//2

    # Cut image
    cutimg = flat[ypos-50:ypos+50, :]

    # Smash
    cut = np.median(cutimg, axis=0)

    # Set flux threshold
    #srt = np.sort(cutimg.flatten()) # this does not work for sparse fibers
    #thresh = srt[int(cutimg.size*0.95)] / 2. # this does not work for sparse fibers

    thresh = np.max(cut)/20.
    pixels_below_threshold=np.where(cut<thresh)[0]
    if pixels_below_threshold.size>2 :
        values_below_threshold = sigma_clip(cut[pixels_below_threshold],sigma=3,iters=200)
        if values_below_threshold.size>2 :
            rms=np.std(values_below_threshold)
            nsig=7
            new_thresh=max(thresh,nsig*rms)
            log.info("Threshold: {:f} -> {:f} ({:d}*rms: {:f})".format(thresh,new_thresh,nsig,nsig*rms))
            thresh=new_thresh

    #gdp = cut > thresh
    # Roll to find peaks (simple algorithm)
    #nstep = nwidth // 2
    #for kk in range(-nstep,nstep):
    #    if kk < 0:
    #        test = np.roll(cut,kk) < np.roll(cut,kk+1)
    #    else:
    #        test = np.roll(cut,kk) > np.roll(cut,kk+1)
    #    # Compare
    #    gdp = gdp & test
    #xpk = np.where(gdp)[0]

    # Find clusters of adjacent points
    clusters=[]
    gdp=np.where(cut > thresh)[0]
    cluster=[gdp[0]]
    for i in gdp[1:] :
        if i==cluster[-1]+1 :
            cluster.append(i)
        else :
            clusters.append(cluster)
            cluster=[i]
    clusters.append(cluster)


    log.info("Number of clusters found: {:d}".format(len(clusters)))

    # Record max of each cluster
    xpk=np.zeros((len(clusters)), dtype=np.int64)
    for i in range(len(clusters)) :
        t=np.argmax(cut[clusters[i]])
        xpk[i]=clusters[i][t]

    if debug:
        #pdb.xplot(cut, xtwo=xpk, ytwo=cut[xpk],mtwo='o')
        pdb.set_trace()

    # Book-keeping and some error checking
    if len(xpk) != Nbundle*Nfiber:
        log.warning('Found the wrong number of total fibers: {:d}'.format(len(xpk)))
    else:
        log.info('Found {:d} fibers'.format(len(xpk)))
    # Find bundles
    xsep = np.roll(xpk,-1) - xpk
    medsep = np.median(xsep)
    bundle_ends = np.where(np.abs(xsep-medsep) > 0.5*medsep)[0]
    if len(bundle_ends) != Nbundle:
        log.warning('Found the wrong number of bundles: {:d}'.format(len(bundle_ends)))
    else:
        log.info('Found {:d} bundles'.format(len(bundle_ends)))
    # Confirm correct number of fibers per bundle
    bad = ((bundle_ends+1) % Nfiber) != 0
    if np.sum(bad) > 0:
        log.warning('Wrong number of fibers in a bundle')
        #raise ValueError('Wrong number of fibers in a bundle')

    # Return
    return xpk, ypos, cut


def fit_traces(xset, xerr, func='legendre', order=6, sigrej=20.,
    RMS_TOLER=0.03, verbose=False):
    """Fit the traces
    Default is 6th order Legendre polynomials

    Parameters
    ----------
    xset : ndarray
      traces
    xerr : ndarray
      Error in the trace values (999.=Bad)
    RMS_TOLER : float, optional [0.02]
      Tolerance on size of RMS in fit

    Returns
    -------
    xnew, fits
    xnew : ndarray
      New fit values (without error)
    fits : list
      List of the fit dicts
    """
    log=get_logger()
    ny = xset.shape[0]
    ntrace = xset.shape[1]
    xnew = np.zeros_like(xset)
    fits = []
    yval = np.arange(ny)
    for ii in range(ntrace):
        mask = xerr[:,ii] > 900.
        nmask = np.sum(mask)
        # Fit with rejection
        dfit, mask = dufits.iter_fit(yval, xset[:,ii], func, order, sig_rej=sigrej,
            weights=1./xerr[:,ii], initialmask=mask, maxone=True)#, sigma=xerr[:,ii])
        # Stats on residuals
        nmask_new = np.sum(mask)-nmask
        if nmask_new > 200:
            log.error("Rejected many points ({:d}) in fiber {:d}".format(nmask_new, ii))
        # Save
        xnew[:,ii] = dufits.func_val(yval,dfit)
        fits.append(dfit)
        # Residuas
        gdval = mask==0
        resid = xnew[:,ii][gdval] - xset[:,ii][gdval]
        rms = np.std(resid)
        if verbose:
            print('RMS of FIT= {:g}'.format(rms))
        if rms > RMS_TOLER:
            #from xastropy.xutils import xdebug as xdb
            #xdb.xplot(yval, xnew[:,ii], xtwo=yval[gdval],ytwo=xset[:,ii][gdval], mtwo='o')
            log.error("RMS {:g} exceeded tolerance for fiber {:d}".format(rms, ii))
    # Return
    return xnew, fits


def extract_sngfibers_gaussianpsf(img, img_ivar, xtrc, sigma, box_radius=2, verbose=True):
    """Extract spectrum for fibers one-by-one using a Gaussian PSF

    Parameters
    ----------
    img : ndarray
      Image
    img_ivar : ndarray
      Image inverse variance
    xtrc : ndarray
      fiber trace
    sigma : float
      Gaussian sigma for PSF
    box_radius : int, optional
      Radius for extraction (+/-)

    Returns
    -------
    spec : ndarray
      Extracted spectrum
    """

    # Init
    xpix_img = np.outer(np.ones(img.shape[0]),np.arange(img.shape[1]))
    mask = np.zeros_like(img,dtype=int)
    iy = np.arange(img.shape[0],dtype=int)

    log = get_logger()

    #
    all_spec = np.zeros_like(xtrc)
    cst = 1./np.sqrt(2*np.pi)
    start=0
    for qq in range(xtrc.shape[1]):
        if verbose & (qq % 25 == 0):
            stop=time.time()
            if start>0 :
                log.info("Working on fiber %d of %d (done 25 in %3.2f sec)"%(qq,xtrc.shape[1],stop-start))
            else :
                log.info("Working on fiber %d of %d"%(qq,xtrc.shape[1]))
            start=stop

        # Mask
        mask[:,:] = 0
        ixt = np.round(xtrc[:,qq]).astype(int)
        for jj,ibox in enumerate(range(-box_radius,box_radius+1)):
            ix = ixt + ibox
            mask[iy,ix] = 1
        # Sub-image (for speed, not convenience)
        gdp = np.where(mask == 1)
        minx = np.min(gdp[1])
        maxx = np.max(gdp[1])
        nx = (maxx-minx)+1

        # Generate PSF
        dx_img = xpix_img[:,minx:maxx+1] - np.outer(xtrc[:,qq], np.ones(nx))
        psf = cst*np.exp(-0.5 * (dx_img/sigma[qq])**2)/sigma[qq]

        #dx_img = xpix_img[:,minx:maxx+1] - np.outer(xtrc[:,qq],np.ones(img.shape[1]))
        #g_init = models.Gaussian1D(amplitude=1., mean=0., stddev=sigma[qq])
        #psf = mask * g_init(dx_img)
        # Extract
        #all_spec[:,qq] = np.sum(psf*img,axis=1) / np.sum(psf,axis=1)
        #all_spec[:,qq] = np.sum(psf*img[:,minx:maxx+1],axis=1) / np.sum(psf,axis=1)
        a=np.sum(img_ivar[:,minx:maxx+1]*psf**2,axis=1)
        b=np.sum(img_ivar[:,minx:maxx+1]*psf*img[:,minx:maxx+1],axis=1)
        ok=(a>1.e-6)
        all_spec[ok,qq] = b[ok] / a[ok]

        #import astropy.io.fits as pyfits
        #h=pyfits.HDUList([pyfits.PrimaryHDU(),
        #                  pyfits.ImageHDU(img[:,minx:maxx+1],name="FLUX"),
        #                  pyfits.ImageHDU(img_ivar[:,minx:maxx+1],name="IVAR"),
        #                  pyfits.ImageHDU(psf,name="PSF"),
        #                  pyfits.ImageHDU(a,name="A"),
        #                  pyfits.ImageHDU(b,name="B")])
        #h.writeto("test.fits")
        #sys.exit(12)





    # Return
    return all_spec


def trace_crude_init(image, xinit0, ypass, invvar=None, radius=2.,
    maxshift0=0.5, maxshift=0.15, maxerr=0.2):
#                   xset, xerr, maxerr, maxshift, maxshift0
    """Python port of trace_crude_idl.pro from IDLUTILS

    Modified for initial guess

    Parameters
    ----------
    image : 2D ndarray
      Image for tracing
    xinit : ndarray
      Initial guesses for trace peak at ypass
    ypass : int
      Row for initial guesses

    Returns
    -------
    xset : Trace for each fiber
    xerr : Estimated error in that trace
    """
    # Init
    xinit = xinit0.astype(float)
    #xinit = xinit[0:3]
    ntrace = xinit.size
    ny = image.shape[0]
    xset = np.zeros((ny,ntrace))
    xerr = np.zeros((ny,ntrace))
    if invvar is None:
        invvar = np.zeros_like(image) + 1.

    #
    #  Recenter INITIAL Row for all traces simultaneously
    #
    iy = ypass * np.ones(ntrace,dtype=int)
    xfit,xfiterr = trace_fweight(image, xinit, iy, invvar=invvar, radius=radius)
    # Shift
    xshift = np.clip(xfit-xinit, -1*maxshift0, maxshift0) * (xfiterr < maxerr)
    xset[ypass,:] = xinit + xshift
    xerr[ypass,:] = xfiterr * (xfiterr < maxerr)  + 999.0 * (xfiterr >= maxerr)

    #    /* LOOP FROM INITIAL (COL,ROW) NUMBER TO LARGER ROW NUMBERS */
    for iy in range(ypass+1, ny):
        xinit = xset[iy-1, :]
        ycen = iy * np.ones(ntrace,dtype=int)
        xfit,xfiterr = trace_fweight(image, xinit, ycen, invvar=invvar, radius=radius)
        # Shift
        xshift = np.clip(xfit-xinit, -1*maxshift, maxshift) * (xfiterr < maxerr)
        # Save
        xset[iy,:] = xinit + xshift
        xerr[iy,:] = xfiterr * (xfiterr < maxerr)  + 999.0 * (xfiterr >= maxerr)
    #      /* LOOP FROM INITIAL (COL,ROW) NUMBER TO SMALLER ROW NUMBERS */
    for iy in range(ypass-1, -1,-1):
        xinit = xset[iy+1, :]
        ycen = iy * np.ones(ntrace,dtype=int)
        xfit,xfiterr = trace_fweight(image, xinit, ycen, invvar=invvar, radius=radius)
        # Shift
        xshift = np.clip(xfit-xinit, -1*maxshift, maxshift) * (xfiterr < maxerr)
        # Save
        xset[iy,:] = xinit + xshift
        xerr[iy,:] = xfiterr * (xfiterr < maxerr) + 999.0 * (xfiterr >= maxerr)

    return xset, xerr


def trace_fweight(fimage, xinit, ycen=None, invvar=None, radius=2., debug=False):
    '''Python port of trace_fweight.pro from IDLUTILS

    Parameters
    ----------
    fimage: 2D ndarray
      Image for tracing
    xinit: ndarray
      Initial guesses for x-trace
    invvar: ndarray, optional
      Inverse variance array for the image
    radius: float, optional
      Radius for centroiding; default to 3.0
    '''
    # Definitions for Cython
    #cdef int nx,ny,ncen

    # Init
    nx = fimage.shape[1]
    ny = fimage.shape[0]
    ncen = len(xinit)
    # Create xnew, xerr
    xnew = xinit.astype(float)
    xerr = np.zeros(ncen) + 999.

    # ycen
    if ycen is None:
        if ncen != ny:
            raise ValueError('Bad input')
        ycen = np.arange(ny, dtype=int)
    else:
        if len(ycen) != ncen:
            raise ValueError('Bad ycen input.  Wrong length')
    x1 = xinit - radius + 0.5
    x2 = xinit + radius + 0.5
    ix1 = np.floor(x1).astype(int)
    ix2 = np.floor(x2).astype(int)

    fullpix = int(np.maximum(np.min(ix2-ix1)-1,0))
    sumw = np.zeros(ncen)
    sumxw = np.zeros(ncen)
    sumwt = np.zeros(ncen)
    sumsx1 = np.zeros(ncen)
    sumsx2 = np.zeros(ncen)
    qbad = np.array([False]*ncen)

    if invvar is None:
        invvar = np.zeros_like(fimage) + 1.

    # Compute
    for ii in range(0,fullpix+3):
        spot = ix1 - 1 + ii
        ih = np.clip(spot,0,nx-1)
        xdiff = spot - xinit
        #
        wt = np.clip(radius - np.abs(xdiff) + 0.5,0,1) * ((spot >= 0) & (spot < nx))
        sumw = sumw + fimage[ycen,ih] * wt
        sumwt = sumwt + wt
        sumxw = sumxw + fimage[ycen,ih] * xdiff * wt
        var_term = wt**2 / (invvar[ycen,ih] + (invvar[ycen,ih] == 0))
        sumsx2 = sumsx2 + var_term
        sumsx1 = sumsx1 + xdiff**2 * var_term
        #qbad = qbad or (invvar[ycen,ih] <= 0)
        qbad = np.any([qbad, invvar[ycen,ih] <= 0], axis=0)

    if debug:
        pdb.set_trace()

    # Fill up
    good = (sumw > 0) &  (~qbad)
    if np.sum(good) > 0:
        delta_x = sumxw[good]/sumw[good]
        xnew[good] = delta_x + xinit[good]
        xerr[good] = np.sqrt(sumsx1[good] + sumsx2[good]*delta_x**2)/sumw[good]

    bad = np.any([np.abs(xnew-xinit) > radius + 0.5,xinit < radius - 0.5,xinit > nx - 0.5 - radius],axis=0)
    if np.sum(bad) > 0:
        xnew[bad] = xinit[bad]
        xerr[bad] = 999.0

    # Return
    return xnew, xerr


def fix_ycoeff_outliers(xcoeff, ycoeff, deg=5, tolerance=2):
    '''
    Fix outliers in coefficients for wavelength solution, assuming a continuous function of CCD coordinates

    Args:
        xcoeff[nfiber, ncoeff] : 2D array of Legendre coefficients for X(wavelength)
        ycoeff[nfiber, ncoeff] : 2D array of Legendre coefficients for Y(wavelength)

    Options:
        deg : integer degree of polynomial to fit
        tolerance : replace fibers with difference of wavelength solution larger than this number of pixels after interpolation

    Returns:
        new_ycoeff[nfiber, ncoeff] with outliers replaced by interpolations

    For each coefficient, fit a polynomial vs. fiber number with one
    pass of sigma clipping.  Remaining outliers are than replaced with
    the interpolated fit value.
    '''

    log = get_logger()

    nfibers=ycoeff.shape[0]
    if nfibers < 3 :
        log.warning("only {:d} fibers, cannot interpolate coefs".format(nfibers))
        return ycoeff
    deg=min(deg,nfibers-1)

    nwave=ycoeff.shape[1]+1
    wave_nodes = np.linspace(-1,1,nwave)

    # get traces using fit coefs
    x=np.zeros((nfibers,nwave))
    y=np.zeros((nfibers,nwave))

    for i in range(nfibers) :
        x[i] = legval(wave_nodes,xcoeff[i])
        y[i] = legval(wave_nodes,ycoeff[i])

    new_ycoeff=ycoeff.copy()

    bad_fibers=None
    while True : # loop to discard one fiber at a time

        # polynomial fit as a function of x for each wave
        yf=np.zeros((nfibers,nwave))
        xx=2*(x - np.min(x)) / (np.max(x) - np.min(x)) - 1
        for i in range(nwave) :
            c=np.polyfit(xx[:,i], y[:,i], deg)
            yf[:,i]=np.polyval(c, xx[:,i])

        diff=np.max(np.abs(y-yf),axis=1)

        for f in range(nfibers) :
            log.info("fiber {:d} maxdiff= {:f}".format(f,diff[f]))


        worst = np.argmax(diff)
        if diff[worst] > tolerance :
            log.warning("replace fiber {:d} trace by interpolation".format(worst))
            leg_fit = dufits.func_fit(wave_nodes, yf[worst], 'legendre', ycoeff.shape[1]-1, xmin=-1, xmax=1)
            new_ycoeff[worst] = leg_fit['coeff']
            y[worst] = legval(wave_nodes,new_ycoeff[worst])
            if bad_fibers is None :
                bad_fibers = np.array([worst])
            else :
                bad_fibers=np.append(bad_fibers, worst)
                bad_fibers=np.unique(bad_fibers)
            continue
        break

    return new_ycoeff


#####################################################################
#####################################################################
# Output
#####################################################################

def write_psf(outfile, xfit, fdicts, gauss, wv_solns, legendre_deg=5, without_arc=False,
              XCOEFF=None, fiberflat_header=None, arc_header=None):
    """ Write the output to a Base PSF format

    Parameters
    ----------
    outfile : str
      Output file
    xfit : ndarray
      Traces
    gauss : list
      List of gaussian sigmas
    fdicts : list
      List of trace fits
    wv_solns : list
      List of wavelength calibrations
    ncoeff : int
      Number of Legendre coefficients in fits
    """
    #

    # check legendre degree makes sense based on number of lines
    if not without_arc:
        nlines=10000
        for ii,id_dict in enumerate(wv_solns):
            if len(id_dict['id_pix']) > 0 :
                nlines_in_fiber=(np.array(id_dict['id_pix'])[id_dict['mask']==0]).size
                #print("fiber #%d nlines=%d"%(ii,nlines_in_fiber))
                nlines=min(nlines,nlines_in_fiber)
        if nlines < legendre_deg+2 :
            legendre_deg=nlines-2
            print("reducing legendre degree to %d because the min. number of emission lines found is %d"%(legendre_deg,nlines))

    ny = xfit.shape[0]
    nfiber = xfit.shape[1]
    ncoeff=legendre_deg+1
    if XCOEFF is None:
        XCOEFF = np.zeros((nfiber, ncoeff))
    YCOEFF = np.zeros((nfiber, ncoeff))

    # Find WAVEMIN, WAVEMAX
    if without_arc:
        WAVEMIN = 0.
        WAVEMAX = ny-1.
        wv_solns = [None]*nfiber
    else:
        WAVEMIN = 10000000.
        WAVEMAX = 0.
        for id_dict in wv_solns :
            if 'wave_min' in id_dict :
                WAVEMIN = min(WAVEMIN,id_dict['wave_min'])
            if 'wave_max' in id_dict :
                WAVEMAX = max(WAVEMAX,id_dict['wave_max'])
        WAVEMIN -= 1.
        WAVEMAX += 1.

    wv_array = np.linspace(WAVEMIN, WAVEMAX, num=ny)
    # Fit Legendre to y vs. wave
    for ii,id_dict in enumerate(wv_solns):



        # Fit y vs. wave
        if without_arc:
            yleg_fit, mask = dufits.iter_fit(wv_array, np.arange(ny), 'legendre', ncoeff-1, xmin=WAVEMIN, xmax=WAVEMAX, niter=1)
        else:
            if len(id_dict['id_wave']) > 0 :
                yleg_fit, mask = dufits.iter_fit(np.array(id_dict['id_wave'])[id_dict['mask']==0], np.array(id_dict['id_pix'])[id_dict['mask']==0], 'legendre', ncoeff-1, xmin=WAVEMIN, xmax=WAVEMAX, sig_rej=100000.)
            else :
                yleg_fit = None
                mask = None

        if yleg_fit is None :
            continue

        YCOEFF[ii, :] = yleg_fit['coeff']

        # Fit x vs. wave
        yval = dufits.func_val(wv_array, yleg_fit)
        if fdicts is None:
            if XCOEFF is None:
                raise IOError("Need to set either fdicts or XCOEFF!")
        else:
            xtrc = dufits.func_val(yval, fdicts[ii])
            xleg_fit,mask = dufits.iter_fit(wv_array, xtrc, 'legendre', ncoeff-1, xmin=WAVEMIN, xmax=WAVEMAX, niter=5, sig_rej=100000.)
            XCOEFF[ii, :] = xleg_fit['coeff']

    # Fix outliers assuming that coefficients vary smoothly vs. CCD coordinates
    YCOEFF = fix_ycoeff_outliers(XCOEFF,YCOEFF,tolerance=2)

    # Write the FITS file
    prihdu = fits.PrimaryHDU(XCOEFF)
    prihdu.header['WAVEMIN'] = WAVEMIN
    prihdu.header['WAVEMAX'] = WAVEMAX
    prihdu.header['EXTNAME'] = 'XTRACE'
    prihdu.header['PSFTYPE'] = 'bootcalib'

    from desiutil.depend import add_dependencies
    add_dependencies(prihdu.header)

    # Add informations for headers
    if arc_header is not None :
        if "NIGHT" in arc_header:
            prihdu.header["ARCNIGHT"] = arc_header["NIGHT"]
        if "EXPID" in arc_header:
            prihdu.header["ARCEXPID"] = arc_header["EXPID"]
        if "CAMERA" in arc_header:
            prihdu.header["CAMERA"] = arc_header["CAMERA"]
        prihdu.header['NPIX_X'] = arc_header['NAXIS1']
        prihdu.header['NPIX_Y'] = arc_header['NAXIS2']
    if fiberflat_header is not None :
        if 'NPIX_X' not in prihdu.header:
            prihdu.header['NPIX_X'] = fiberflat_header['NAXIS1']
            prihdu.header['NPIX_Y'] = fiberflat_header['NAXIS2']
        if "NIGHT" in fiberflat_header:
            prihdu.header["FLANIGHT"] = fiberflat_header["NIGHT"]
        if "EXPID" in fiberflat_header:
            prihdu.header["FLAEXPID"] = fiberflat_header["EXPID"]

    yhdu = fits.ImageHDU(YCOEFF, name='YTRACE')

    # also save wavemin wavemax in yhdu
    yhdu.header['WAVEMIN'] = WAVEMIN
    yhdu.header['WAVEMAX'] = WAVEMAX

    gausshdu = fits.ImageHDU(np.array(gauss), name='XSIGMA')

    hdulist = fits.HDUList([prihdu, yhdu, gausshdu])


    hdulist.writeto(outfile, clobber=True)

def write_line_list(filename,all_wv_soln,llist) :
    wave = np.array([])
    for id_dict in all_wv_soln :
        wave=np.append(wave,id_dict["id_wave"])
    wave=np.unique(wave)

    ofile=open(filename,"w")
    ofile.write("# from bootcalib\n")
    ofile.write("Ion wave score RelInt\n")
    for w in wave :
        ii=np.argmin(np.abs(llist["wave"]-w))
        print(w,llist["wave"][ii],llist["Ion"][ii])
        ofile.write("{:s} {:f} 1 1\n".format(llist["Ion"][ii],w))
    ofile.close()





#####################################################################
#####################################################################
# Utilities
#####################################################################

def script_bootcalib(arc_idx, flat_idx, cameras=None, channels=None, nproc=10):
    """ Runs desi_bootcalib on a series of pix files

    Returns:
        script_bootcalib([0,1,2,3,4,5,6,7,8,9], [10,11,12,13,14])

    """
    from subprocess import Popen
    #
    if cameras is None:
        cameras = ['0','1','2','3','4','5','6','7','8','9']
    if channels is None:
        channels = ['b','r','z']
        #channels = ['b']#,'r','z']
    nchannels = len(channels)
    ncameras = len(cameras)
    #
    narc = len(arc_idx)
    nflat = len(flat_idx)
    ntrial = narc*nflat*ncameras*nchannels

    # Loop on the systems
    nrun = -1
    #nrun = 123
    while(nrun < ntrial):

        proc = []
        ofiles = []
        for ss in range(nproc):
            nrun += 1
            iarc = nrun % narc
            jflat = (nrun//narc) % nflat
            kcamera = (nrun//(narc*nflat)) % ncameras
            lchannel = nrun // (narc*nflat*ncameras)
            #pdb.set_trace()
            if nrun == ntrial:
                break
            # Names
            afile = str('pix-{:s}{:s}-{:08d}.fits'.format(channels[lchannel], cameras[kcamera], arc_idx[iarc]))
            ffile = str('pix-{:s}{:s}-{:08d}.fits'.format(channels[lchannel], cameras[kcamera], flat_idx[jflat]))
            ofile = str('boot_psf-{:s}{:s}-{:d}{:d}.fits'.format(channels[lchannel], cameras[kcamera],
                                                                 arc_idx[iarc], flat_idx[jflat]))
            qfile = str('qa_boot-{:s}{:s}-{:d}{:d}.pdf'.format(channels[lchannel], cameras[kcamera],
                                                                 arc_idx[iarc], flat_idx[jflat]))
            lfile = str('boot-{:s}{:s}-{:d}{:d}.log'.format(channels[lchannel], cameras[kcamera],
                                                               arc_idx[iarc], flat_idx[jflat]))
            ## Run
            script = [str('desi_bootcalib.py'), str('--fiberflat={:s}'.format(ffile)),
                      str('--arcfile={:s}'.format(afile)),
                      str('--outfile={:s}'.format(ofile)),
                      str('--qafile={:s}'.format(qfile))]#,
                      #str('>'),
                      #str('{:s}'.format(lfile))]
            f = open(lfile, "w")
            proc.append(Popen(script, stdout=f))
            ofiles.append(f)
        exit_codes = [p.wait() for p in proc]
        for ofile in ofiles:
            ofile.close()


#####################################################################
#####################################################################
#####################################################################
# QA
#####################################################################

def qa_fiber_peaks(xpk, cut, pp=None, figsz=None, nper=100):
    """ Generate a QA plot for the fiber peaks

    Args:
        xpk: x positions on the CCD of the fiber peaks at a ypos
        cut: Spatial cut through the detector
        pp: PDF file pointer
        figsz: figure size, optional
        nper: number of fibers per row in the plot, optional

    """
    # Init
    if figsz is None:
        figsz = glbl_figsz

    nfiber = xpk.size
    nrow = (nfiber // nper) + ((nfiber % nper) > 0)
    xplt = np.arange(cut.size)
    # Plots
    gs = gridspec.GridSpec(nrow, 1)
    plt.figure(figsize=figsz)
    # Loop
    for ii in range(nrow):
        ax = plt.subplot(gs[ii])
        i0 = ii*nper
        i1 = i0 + nper
        ax.plot(xplt,cut, 'k-')
        ax.plot(xpk, cut[xpk],'go')
        xmin = np.min(xpk[i0:i1])-10.
        xmax = np.max(xpk[i0:i1])+10.
        ax.set_xlim(xmin,xmax)
    # Save and close
    if pp is not None:
        pp.savefig(bbox_inches='tight')
    else:
        plt.show()
    plt.close()


def qa_fiber_Dx(xfit, fdicts, pp=None, figsz=None):
    """ Show the spread in the trace per fiber

    Used to diagnose the traces

    Args:
        xfit: traces
        fdicts: dict of the traces
        pp: PDF file pointer
        figsz: figure size, optional

    """
    #
    if figsz is None:
        figsz = glbl_figsz
    # Calculate Dx
    nfiber = xfit.shape[1]
    Dx = []
    for ii in range(nfiber):
        Dx.append(np.max(xfit[:, ii])-np.min(xfit[:, ii]))
    # Plot
    plt.figure(figsize=figsz)
    plt.scatter(np.arange(nfiber), np.array(Dx))
    # Label
    plt.xlabel('Fiber', fontsize=17.)
    plt.ylabel(r'$\Delta x$ (pixels)', fontsize=17.)
    # Save and close
    if pp is None:
        plt.show()
    else:
        pp.savefig(bbox_inches='tight')
    plt.close()

def qa_fiber_gauss(gauss, pp=None, figsz=None):
    """ Show the Gaussian (sigma) fits to each fiber

    Args:
        gauss: Gaussian of each fiber
        pp: PDF file pointer
        figsz: figure size, optional

    """
    #
    if figsz is None:
        figsz = glbl_figsz
    # Calculate Dx
    nfiber = gauss.size
    # Plot
    plt.figure(figsize=figsz)
    plt.scatter(np.arange(nfiber), gauss)
    # Label
    plt.xlabel('Fiber', fontsize=17.)
    plt.ylabel('Gaussian sigma (pixels)', fontsize=17.)
    # Save and close
    if pp is None:
        plt.show()
    else:
        pp.savefig(bbox_inches='tight')
    plt.close()

def qa_arc_spec(all_spec, all_soln, pp, figsz=None):
    """ Generate QA plots of the arc spectra with IDs

    Args:
        all_spec: Arc 1D fiber spectra
        all_soln: Wavelength solutions
        pp: PDF file pointer
        figsz: figure size, optional

    """
    # Init
    if figsz is None:
        figsz = glbl_figsz
    nfiber = len(all_soln)
    npix = all_spec.shape[0]
    #
    nrow = 2
    ncol = 3
    # Plots
    gs = gridspec.GridSpec(nrow, ncol)
    plt.figure(figsize=figsz)
    # Loop
    for ii in range(nrow*ncol):
        ax = plt.subplot(gs[ii])
        idx = ii * (nfiber//(nrow*ncol))
        yspec = np.log10(np.maximum(all_spec[:,idx],1))
        ax.plot(np.arange(npix), yspec, 'k-')
        ax.set_xlabel('Pixel')
        ax.set_ylabel('log Flux')
        # ID
        id_dict = all_soln[idx]
        for jj,xpixpk in enumerate(id_dict['id_pix']):
            ax.text(xpixpk, yspec[int(np.round(xpixpk))], '{:g}'.format(id_dict['id_wave'][jj]), ha='center',color='red', rotation=90.)

    # Save and close
    pp.savefig(bbox_inches='tight')
    plt.close()


def qa_fiber_arcrms(all_soln, pp, figsz=None):
    """ Show the RMS of the wavelength solutions vs. fiber

    Args:
        all_soln: Wavelength solutions
        pp: PDF file pointer
        figsz: figure size, optional

    """
    #
    if figsz is None:
        figsz = glbl_figsz
    # Calculate Dx
    nfiber = len(all_soln)
    rms = [id_dict['rms'] for id_dict in all_soln]
    # Plot
    plt.figure(figsize=figsz)
    plt.scatter(np.arange(nfiber), np.array(rms))
    # Label
    plt.xlabel('Fiber', fontsize=17.)
    plt.ylabel('RMS (pixels)', fontsize=17.)
    # Save and close
    pp.savefig(bbox_inches='tight')
    plt.close()


def qa_fiber_dlamb(all_spec, all_soln, pp, figsz=None):
    """ Show the Dlamb of the wavelength solutions vs. fiber

    Args:
        all_soln: Wavelength solutions
        pp: PDF file pointer
        figsz: figure size, optional

    """
    #
    if figsz is None:
        figsz = glbl_figsz
    # Calculate Dx
    nfiber = len(all_soln)
    npix = all_spec.shape[0]
    xval = np.arange(npix)
    dlamb = []
    for ii in range(nfiber):
        idict = all_soln[ii]
        wave = dufits.func_val(xval,idict['final_fit_pix'])
        dlamb.append(np.median(np.abs(wave-np.roll(wave,1))))
    # Plot
    plt.figure(figsize=figsz)
    plt.scatter(np.arange(nfiber), np.array(dlamb))
    # Label
    plt.xlabel('Fiber', fontsize=17.)
    plt.ylabel(r'$\Delta \lambda$ (Ang)', fontsize=17.)
    # Save and close
    pp.savefig(bbox_inches='tight')
    plt.close()


def qa_fiber_trace(flat, xtrc, outfil=None, Nfiber=25, isclmin=0.5):
    ''' Generate a QA plot for the fiber traces

    Parameters
    ----------
    flat: ndarray
      image
    xtrc: ndarray
      Trace array
    isclmin: float, optional [0.5]
      Fraction of 90 percentile flux to scale image by
    outfil: str, optional
      Output file
    normalize: bool, optional
      Normalize the flat?  If not, use zscale for output
    '''

    ticks_font = matplotlib.font_manager.FontProperties(family='times new roman',
       style='normal', size=16, weight='normal', stretch='normal')
    plt.rcParams['font.family']= 'times new roman'
    cmm = cm.Greys_r
    # Outfil
    if outfil is None:
        outfil = 'fiber_trace_qa.pdf'
    ntrc = xtrc.shape[1]
    ycen = np.arange(flat.shape[0])

    # Plot
    pp = PdfPages(outfil)
    plt.clf()
    fig = plt.figure(figsize=(8, 5.0),dpi=1200)
    #fig.set_size_inches(10.0,6.5)
    Nbundle = ntrc // Nfiber + (ntrc%Nfiber > 0)
    for qq in range(Nbundle):
        ax = plt.gca()
        for label in ax.get_yticklabels() :
            label.set_fontproperties(ticks_font)
        for label in ax.get_xticklabels() :
            label.set_fontproperties(ticks_font)

        # Cut image
        i0 = qq*Nfiber
        i1 = np.minimum((qq+1)*Nfiber,ntrc)
        x0 = np.maximum(int(np.min(xtrc[:,i0:i1]))-3,0)
        x1 = np.minimum(int(np.max(xtrc[:,i0:i1]))+3,flat.shape[1])
        sub_flat = flat[:,x0:x1].T
        # Scale
        srt = np.sort(sub_flat.flatten())
        sclmax = srt[int(sub_flat.size*0.9)]
        sclmin = isclmin * sclmax
        # Plot
        mplt = plt.imshow(sub_flat,origin='lower', cmap=cmm,
            extent=(0., sub_flat.shape[1]-1, x0,x1-1), aspect='auto')
            #extent=(0., sub_flat.shape[1]-1, x0,x1))
        #mplt.set_clim(vmin=sclmin, vmax=sclmax)

        # Axes
        #plt.xlim(0., sub_flat.shape[1]-1)
        plt.xlim(0., sub_flat.shape[1]-1)
        plt.ylim(x0,x1)

        # Traces
        for ii in range(i0,i1):
            # Left
            plt.plot(ycen, xtrc[:,ii], 'r-',alpha=0.7, linewidth=0.5)
            # Label
            #iy = int(frame.shape[0]/2.)
            #plt.text(ltrace[iy,ii], ycen[iy], '{:d}'.format(ii+1), color='red', ha='center')
            #plt.text(rtrace[iy,ii], ycen[iy], '{:d}'.format(ii+1), color='green', ha='center')

        pp.savefig(bbox_inches='tight')
        plt.close()
    # Finish
    print('Writing {:s} QA for fiber trace'.format(outfil))
    pp.close()
